{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:08:46.724537Z",
     "iopub.status.busy": "2025-06-14T16:08:46.723796Z",
     "iopub.status.idle": "2025-06-14T16:09:05.038901Z",
     "shell.execute_reply": "2025-06-14T16:09:05.037861Z",
     "shell.execute_reply.started": "2025-06-14T16:08:46.724480Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-14 16:08:53.024311: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-06-14 16:08:53.045812: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-06-14 16:08:53.052460: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    }
   ],
   "source": [
    "!python3 -c \"import monai\" || pip3 install -q \"monai-weekly[nibabel, tqdm, einops]\" \n",
    "!python3 -c \"import matplotlib\" || pip3 install -q matplotlib \n",
    "%matplotlib inline\n",
    "!pip3 install -q einops \n",
    "!pip install -q wandb\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:05.040715Z",
     "iopub.status.busy": "2025-06-14T16:09:05.040446Z",
     "iopub.status.idle": "2025-06-14T16:09:05.046460Z",
     "shell.execute_reply": "2025-06-14T16:09:05.045703Z",
     "shell.execute_reply.started": "2025-06-14T16:09:05.040694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import tempfile\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from monai.apps import DecathlonDataset\n",
    "from monai.config import print_config\n",
    "from monai.data import DataLoader, decollate_batch\n",
    "from monai.handlers.utils import from_engine\n",
    "from monai.losses import DiceLoss\n",
    "from monai.inferers import sliding_window_inference\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    Activationsd,\n",
    "    AsDiscrete,\n",
    "    AsDiscreted,\n",
    "    Compose,\n",
    "    Invertd,\n",
    "    LoadImaged,\n",
    "    MapTransform,\n",
    "    NormalizeIntensityd,\n",
    "    Orientationd,\n",
    "    RandFlipd,\n",
    "    RandScaleIntensityd,\n",
    "    RandShiftIntensityd,\n",
    "    RandSpatialCropd,\n",
    "    Spacingd,\n",
    "    EnsureTyped,\n",
    "    EnsureChannelFirstd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:05.048584Z",
     "iopub.status.busy": "2025-06-14T16:09:05.048245Z",
     "iopub.status.idle": "2025-06-14T16:09:13.564978Z",
     "shell.execute_reply": "2025-06-14T16:09:13.564283Z",
     "shell.execute_reply.started": "2025-06-14T16:09:05.048552Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33myokai-re77\u001b[0m (\u001b[33myokai-re77-north-south-university\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "user_secrets = UserSecretsClient()\n",
    "api_key = user_secrets.get_secret(\"wandb\")\n",
    "\n",
    "import wandb\n",
    "wandb.login(key=api_key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating JSON for Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:13.566475Z",
     "iopub.status.busy": "2025-06-14T16:09:13.565908Z",
     "iopub.status.idle": "2025-06-14T16:09:13.594894Z",
     "shell.execute_reply": "2025-06-14T16:09:13.594296Z",
     "shell.execute_reply.started": "2025-06-14T16:09:13.566446Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Get sorted file paths and file names\n",
    "file_paths1 = glob.glob('/kaggle/input/brats2023-part-1/*')  # Fixed the glob pattern\n",
    "file_paths1.sort()\n",
    "\n",
    "file_names1 = [os.path.basename(path) for path in file_paths1]  # Extract file names from paths\n",
    "file_names1.sort()\n",
    "\n",
    "# Initialize lists for different MRI modalities and segmentation labels\n",
    "t1c, t1n, t2f, t2w, label = [], [], [], [], []\n",
    "\n",
    "# Use the total number of files instead of a fixed 330\n",
    "num_files = len(file_paths1)\n",
    "\n",
    "# Populate the lists with file paths\n",
    "for i in range(num_files):\n",
    "    t1c.append(os.path.join(file_paths1[i], file_names1[i] + '-t1c.nii'))\n",
    "    t1n.append(os.path.join(file_paths1[i], file_names1[i] + '-t1n.nii'))\n",
    "    t2f.append(os.path.join(file_paths1[i], file_names1[i] + '-t2f.nii'))\n",
    "    t2w.append(os.path.join(file_paths1[i], file_names1[i] + '-t2w.nii'))\n",
    "    label.append(os.path.join(file_paths1[i], file_names1[i] + '-seg.nii'))\n",
    "\n",
    "# Store in a dictionary with combined image modalities and separate label\n",
    "file_list = []\n",
    "for i in range(num_files):\n",
    "    file_list.append({\n",
    "        \"image\": [t1c[i], t1n[i], t2f[i], t2w[i]],  # Combine modalities into one \"image\" field\n",
    "        \"label\": label[i]\n",
    "    })\n",
    "\n",
    "file_json = {\n",
    "    \"training\": file_list\n",
    "}\n",
    "\n",
    "# Save to JSON file\n",
    "file_path = '/kaggle/working/dataset.json'\n",
    "with open(file_path, 'w') as json_file:\n",
    "    json.dump(file_json, json_file, indent=4)\n",
    "\n",
    "# Print the first 100 entries\n",
    "# print(json.dumps({\"training\": file_list[:100]}, indent=4))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MONAI Directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:13.595745Z",
     "iopub.status.busy": "2025-06-14T16:09:13.595553Z",
     "iopub.status.idle": "2025-06-14T16:09:13.600103Z",
     "shell.execute_reply": "2025-06-14T16:09:13.599339Z",
     "shell.execute_reply.started": "2025-06-14T16:09:13.595729Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/working/\n"
     ]
    }
   ],
   "source": [
    "directory = os.environ.get(\"MONAI_DATA_DIRECTORY\")\n",
    "root_dir = '/kaggle/working/'\n",
    "print(root_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:13.601365Z",
     "iopub.status.busy": "2025-06-14T16:09:13.601091Z",
     "iopub.status.idle": "2025-06-14T16:09:13.620053Z",
     "shell.execute_reply": "2025-06-14T16:09:13.619248Z",
     "shell.execute_reply.started": "2025-06-14T16:09:13.601338Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "set_determinism(seed=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert Labels to BRATS 2023"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:13.621143Z",
     "iopub.status.busy": "2025-06-14T16:09:13.620875Z",
     "iopub.status.idle": "2025-06-14T16:09:13.625953Z",
     "shell.execute_reply": "2025-06-14T16:09:13.625312Z",
     "shell.execute_reply.started": "2025-06-14T16:09:13.621118Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ConvertLabels(MapTransform):\n",
    "    \"\"\"\n",
    "#     Convert labels to multi channels based on BRATS 2023 classes:\n",
    "#     label 1 is Necrotic Tumor Core (NCR)\n",
    "#     label 2 is Edema (ED)\n",
    "#     label 3 is Enhancing Tumor (ET)\n",
    "#     label 0 is everything else (non-tumor)\n",
    "#     \"\"\"\n",
    "\n",
    "    def __call__(self, data):\n",
    "        d = dict(data)\n",
    "        for key in self.keys:\n",
    "            result = []\n",
    "            # Tumor Core (TC) = NCR + Enhancing Tumor (ET)\n",
    "            result.append(torch.logical_or(d[key] == 1, d[key] == 3))\n",
    "            # Whole Tumor (WT) = NCR + Edema + Enhancing Tumor\n",
    "            result.append(torch.logical_or(torch.logical_or(d[key] == 1, d[key] == 2), d[key] == 3))\n",
    "            # Enhancing Tumor (ET) = Enhancing Tumor (label 3)\n",
    "            result.append(d[key] == 3)\n",
    "            d[key] = torch.stack(result, axis=0).float()\n",
    "        return d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Data and Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:13.628520Z",
     "iopub.status.busy": "2025-06-14T16:09:13.628240Z",
     "iopub.status.idle": "2025-06-14T16:09:13.661437Z",
     "shell.execute_reply": "2025-06-14T16:09:13.660836Z",
     "shell.execute_reply.started": "2025-06-14T16:09:13.628478Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Optimized Transforms for Brain Tumor Segmentation - Dice Score Focused\n",
    "\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, EnsureTyped,\n",
    "    Orientationd, Spacingd, RandSpatialCropd, RandFlipd, NormalizeIntensityd,\n",
    "    RandScaleIntensityd, RandShiftIntensityd, RandRotate90d, RandGaussianNoised,\n",
    "    RandAdjustContrastd, RandAffined\n",
    ")\n",
    "\n",
    "# Optimized Training Transforms - Only Dice-Boosting Augmentations\n",
    "train_transform = Compose(\n",
    "    [\n",
    "        # Essential loading and preprocessing\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertLabels(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        \n",
    "        # Spatial cropping\n",
    "        RandSpatialCropd(keys=[\"image\", \"label\"], roi_size=[96, 96, 96], random_size=False),\n",
    "        \n",
    "        # Geometric augmentations - proven to boost dice\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=0),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=1),\n",
    "        RandFlipd(keys=[\"image\", \"label\"], prob=0.5, spatial_axis=2),\n",
    "        RandRotate90d(keys=[\"image\", \"label\"], prob=0.3, spatial_axes=(0, 1)),\n",
    "        \n",
    "        # Conservative affine transformations\n",
    "        RandAffined(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "            prob=0.4,\n",
    "            rotate_range=(0.1, 0.1, 0.1),  # Reduced rotation\n",
    "            scale_range=(0.05, 0.05, 0.05),  # Reduced scaling\n",
    "            translate_range=(5, 5, 5),  # Small translations\n",
    "        ),\n",
    "        \n",
    "        # Essential intensity normalization\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "        \n",
    "        # Conservative intensity augmentations\n",
    "        RandScaleIntensityd(keys=\"image\", factors=0.1, prob=0.8),  # Reduced prob\n",
    "        RandShiftIntensityd(keys=\"image\", offsets=0.1, prob=0.8),  # Reduced prob\n",
    "        \n",
    "        # Light Gaussian noise for robustness\n",
    "        RandGaussianNoised(keys=\"image\", prob=0.2, std=0.01),\n",
    "        \n",
    "        # Contrast adjustment - helps with tumor boundary detection\n",
    "        RandAdjustContrastd(keys=\"image\", prob=0.3, gamma=(0.8, 1.3)),  # Conservative range\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation Transform (no augmentation)\n",
    "val_transform = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=[\"image\", \"label\"]),\n",
    "        EnsureChannelFirstd(keys=\"image\"),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "        ConvertLabels(keys=\"label\"),\n",
    "        Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "        Spacingd(\n",
    "            keys=[\"image\", \"label\"],\n",
    "            pixdim=(1.0, 1.0, 1.0),\n",
    "            mode=(\"bilinear\", \"nearest\"),\n",
    "        ),\n",
    "        NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset and Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:13.663217Z",
     "iopub.status.busy": "2025-06-14T16:09:13.663026Z",
     "iopub.status.idle": "2025-06-14T16:09:13.684403Z",
     "shell.execute_reply": "2025-06-14T16:09:13.683685Z",
     "shell.execute_reply.started": "2025-06-14T16:09:13.663202Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import monai\n",
    "from monai.transforms import (\n",
    "    Compose, LoadImaged, EnsureChannelFirstd, EnsureTyped, Orientationd,\n",
    "    Spacingd, RandSpatialCropd, RandFlipd, NormalizeIntensityd,\n",
    "    RandScaleIntensityd, RandShiftIntensityd\n",
    ")\n",
    "from monai.data import Dataset, DataLoader, CacheDataset, PersistentDataset\n",
    "from monai.utils import set_determinism\n",
    "from sklearn.model_selection import train_test_split\n",
    "import json\n",
    "\n",
    "# Load dataset\n",
    "dataset_path = \"/kaggle/working/dataset.json\"\n",
    "with open(dataset_path) as f:\n",
    "    datalist = json.load(f)[\"training\"]\n",
    "\n",
    "# Split dataset into training (80%) and validation (20%)\n",
    "train_files, val_files = train_test_split(datalist, test_size=0.2, random_state=42)\n",
    "\n",
    "### For quick iterations use 10 percent dataset\n",
    "\n",
    "# train_files = train_files[:int(len(train_files) * 0.3)]  # 10% of training data\n",
    "# val_files = val_files[:int(len(val_files) * 0.3)]  # 10% of validation data\n",
    "\n",
    "# Set deterministic behavior\n",
    "set_determinism(seed=0)\n",
    "\n",
    "cache_dir = \"/kaggle/working/cache\"\n",
    "\n",
    "\n",
    "train_transform = train_transform\n",
    "val_transform = val_transform\n",
    "\n",
    "# Create MONAI datasets\n",
    "\n",
    "train_ds = Dataset(data=train_files, transform=train_transform)\n",
    "val_ds = Dataset(data=val_files, transform=val_transform)\n",
    "\n",
    "# train_ds = Dataset(data=train_files[:int(0.3 * len(train_files))], transform=train_transform)\n",
    "# val_ds = Dataset(data=val_files[:int(0.3 * len(val_files))], transform=val_transform)\n",
    "\n",
    "# Dataloaders\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=3, pin_memory=True, persistent_workers=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=3, pin_memory=True, persistent_workers=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Using Torch Lightning Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:13.685466Z",
     "iopub.status.busy": "2025-06-14T16:09:13.685271Z",
     "iopub.status.idle": "2025-06-14T16:09:13.689410Z",
     "shell.execute_reply": "2025-06-14T16:09:13.688433Z",
     "shell.execute_reply.started": "2025-06-14T16:09:13.685448Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import multiprocessing\n",
    "\n",
    "torch.multiprocessing.set_sharing_strategy(\"file_system\")\n",
    "multiprocessing.set_start_method('fork', force=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:13.690502Z",
     "iopub.status.busy": "2025-06-14T16:09:13.690243Z",
     "iopub.status.idle": "2025-06-14T16:09:13.703769Z",
     "shell.execute_reply": "2025-06-14T16:09:13.702946Z",
     "shell.execute_reply.started": "2025-06-14T16:09:13.690455Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# !rm -rf /kaggle/working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:13.704783Z",
     "iopub.status.busy": "2025-06-14T16:09:13.704511Z",
     "iopub.status.idle": "2025-06-14T16:09:13.718083Z",
     "shell.execute_reply": "2025-06-14T16:09:13.717419Z",
     "shell.execute_reply.started": "2025-06-14T16:09:13.704761Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Restart the kernel (THIS WILL INTERRUPT THE KERNEL)\n",
    "# import os\n",
    "# os._exit(00)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MedNext Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:09:13.719280Z",
     "iopub.status.busy": "2025-06-14T16:09:13.718974Z",
     "iopub.status.idle": "2025-06-14T16:09:13.739997Z",
     "shell.execute_reply": "2025-06-14T16:09:13.739048Z",
     "shell.execute_reply.started": "2025-06-14T16:09:13.719253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from typing import Sequence\n",
    "\n",
    "# MedNeXt Components (Fixed Version)\n",
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm that supports two data formats: channels_last (default) or channels_first. \n",
    "    The ordering of the dimensions in the inputs. channels_last corresponds to inputs with \n",
    "    shape (batch_size, height, width, channels) while channels_first corresponds to inputs \n",
    "    with shape (batch_size, channels, height, width).\n",
    "    \"\"\"\n",
    "    def __init__(self, normalized_shape, eps=1e-5, data_format=\"channels_last\"):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(normalized_shape))        # beta\n",
    "        self.bias = nn.Parameter(torch.zeros(normalized_shape))         # gamma\n",
    "        self.eps = eps\n",
    "        self.data_format = data_format\n",
    "        if self.data_format not in [\"channels_last\", \"channels_first\"]:\n",
    "            raise NotImplementedError \n",
    "        self.normalized_shape = (normalized_shape, )\n",
    "    \n",
    "    def forward(self, x, dummy_tensor=False):\n",
    "        if self.data_format == \"channels_last\":\n",
    "            return F.layer_norm(x, self.normalized_shape, self.weight, self.bias, self.eps)\n",
    "        elif self.data_format == \"channels_first\":\n",
    "            u = x.mean(1, keepdim=True)\n",
    "            s = (x - u).pow(2).mean(1, keepdim=True)\n",
    "            x = (x - u) / torch.sqrt(s + self.eps)\n",
    "            x = self.weight[:, None, None, None] * x + self.bias[:, None, None, None]\n",
    "            return x\n",
    "\n",
    "class MedNeXtBlock(nn.Module):\n",
    "    def __init__(self, \n",
    "                in_channels: int, \n",
    "                out_channels: int, \n",
    "                exp_r: int = 4, \n",
    "                kernel_size: int = 7, \n",
    "                do_res: bool = True,\n",
    "                norm_type: str = 'group',\n",
    "                n_groups: int = None,\n",
    "                dim: str = '3d',\n",
    "                grn: bool = False\n",
    "                ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.do_res = do_res and (in_channels == out_channels)  # Only do residual if channels match\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "\n",
    "        assert dim in ['2d', '3d']\n",
    "        self.dim = dim\n",
    "        if self.dim == '2d':\n",
    "            conv = nn.Conv2d\n",
    "        elif self.dim == '3d':\n",
    "            conv = nn.Conv3d\n",
    "            \n",
    "        # First convolution layer with DepthWise Convolutions\n",
    "        self.conv1 = conv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=in_channels,\n",
    "            kernel_size=kernel_size,\n",
    "            stride=1,\n",
    "            padding=kernel_size//2,\n",
    "            groups=in_channels if n_groups is None else n_groups,\n",
    "        )\n",
    "\n",
    "        # Normalization Layer. GroupNorm is used by default.\n",
    "        if norm_type == 'group':\n",
    "            self.norm = nn.GroupNorm(\n",
    "                num_groups=in_channels, \n",
    "                num_channels=in_channels\n",
    "                )\n",
    "        elif norm_type == 'layer':\n",
    "            self.norm = LayerNorm(\n",
    "                normalized_shape=in_channels, \n",
    "                data_format='channels_first'\n",
    "                )\n",
    "\n",
    "        # Second convolution (Expansion) layer\n",
    "        self.conv2 = conv(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=exp_r*in_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        \n",
    "        # GeLU activations\n",
    "        self.act = nn.GELU()\n",
    "        \n",
    "        # Third convolution (Compression) layer\n",
    "        self.conv3 = conv(\n",
    "            in_channels=exp_r*in_channels,\n",
    "            out_channels=out_channels,\n",
    "            kernel_size=1,\n",
    "            stride=1,\n",
    "            padding=0\n",
    "        )\n",
    "        \n",
    "        # Add projection layer for residual connection when channels don't match\n",
    "        if in_channels != out_channels and do_res:\n",
    "            self.projection = conv(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=1,\n",
    "                stride=1,\n",
    "                padding=0\n",
    "            )\n",
    "        else:\n",
    "            self.projection = None\n",
    "\n",
    "        self.grn = grn\n",
    "        if grn:\n",
    "            if dim == '3d':\n",
    "                self.grn_beta = nn.Parameter(torch.zeros(1, exp_r*in_channels, 1, 1, 1), requires_grad=True)\n",
    "                self.grn_gamma = nn.Parameter(torch.zeros(1, exp_r*in_channels, 1, 1, 1), requires_grad=True)\n",
    "            elif dim == '2d':\n",
    "                self.grn_beta = nn.Parameter(torch.zeros(1, exp_r*in_channels, 1, 1), requires_grad=True)\n",
    "                self.grn_gamma = nn.Parameter(torch.zeros(1, exp_r*in_channels, 1, 1), requires_grad=True)\n",
    "\n",
    "    def forward(self, x, dummy_tensor=None):\n",
    "        residual = x\n",
    "        \n",
    "        x1 = self.conv1(x)\n",
    "        x1 = self.act(self.conv2(self.norm(x1)))\n",
    "        \n",
    "        if self.grn:\n",
    "            if self.dim == '3d':\n",
    "                gx = torch.norm(x1, p=2, dim=(-3, -2, -1), keepdim=True)\n",
    "            elif self.dim == '2d':\n",
    "                gx = torch.norm(x1, p=2, dim=(-2, -1), keepdim=True)\n",
    "            nx = gx / (gx.mean(dim=1, keepdim=True) + 1e-6)\n",
    "            x1 = self.grn_gamma * (x1 * nx) + self.grn_beta + x1\n",
    "            \n",
    "        x1 = self.conv3(x1)\n",
    "        \n",
    "        # Handle residual connection properly\n",
    "        if self.do_res or self.projection is not None:\n",
    "            if self.projection is not None:\n",
    "                residual = self.projection(residual)\n",
    "            x1 = residual + x1  \n",
    "            \n",
    "        return x1\n",
    "\n",
    "class MedNeXtUpBlock(nn.Module):\n",
    "    \"\"\"MedNeXt-based upsampling block for decoder\"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 exp_r: int = 4,\n",
    "                 kernel_size: int = 7,\n",
    "                 norm_type: str = 'group',\n",
    "                 spatial_dims: int = 3,\n",
    "                 grn: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.spatial_dims = spatial_dims\n",
    "        dim = '3d' if spatial_dims == 3 else '2d'\n",
    "        \n",
    "        if spatial_dims == 3:\n",
    "            self.upsample = nn.ConvTranspose3d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            )\n",
    "        else:\n",
    "            self.upsample = nn.ConvTranspose2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels,\n",
    "                kernel_size=2,\n",
    "                stride=2\n",
    "            )\n",
    "        \n",
    "        # MedNeXt block for feature processing\n",
    "        self.mednext_block = MedNeXtBlock(\n",
    "            in_channels=out_channels * 2,  # Concatenated features\n",
    "            out_channels=out_channels,\n",
    "            exp_r=exp_r,\n",
    "            kernel_size=kernel_size,\n",
    "            do_res=False,  # Set to False since input/output channels are different\n",
    "            norm_type=norm_type,\n",
    "            dim=dim,\n",
    "            grn=grn\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, skip):\n",
    "        # Upsample the input\n",
    "        x = self.upsample(x)\n",
    "        \n",
    "        # Concatenate with skip connection\n",
    "        x = torch.cat([x, skip], dim=1)\n",
    "        \n",
    "        # Process through MedNeXt block\n",
    "        x = self.mednext_block(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class MedNeXtEncoder(nn.Module):\n",
    "    \"\"\"MedNeXt-based encoder block\"\"\"\n",
    "    def __init__(self, \n",
    "                 in_channels: int, \n",
    "                 out_channels: int, \n",
    "                 exp_r: int = 4,\n",
    "                 kernel_size: int = 7,\n",
    "                 norm_type: str = 'group',\n",
    "                 spatial_dims: int = 3,\n",
    "                 grn: bool = False):\n",
    "        super().__init__()\n",
    "        \n",
    "        dim = '3d' if spatial_dims == 3 else '2d'\n",
    "        \n",
    "        # MedNeXt blocks for feature processing\n",
    "        # First block handles channel dimension change\n",
    "        self.block1 = MedNeXtBlock(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=out_channels,\n",
    "            exp_r=exp_r,\n",
    "            kernel_size=kernel_size,\n",
    "            do_res=True if in_channels == out_channels else False,  # Only residual if channels match\n",
    "            norm_type=norm_type,\n",
    "            dim=dim,\n",
    "            grn=grn\n",
    "        )\n",
    "        \n",
    "        # Second block maintains channel dimensions\n",
    "        self.block2 = MedNeXtBlock(\n",
    "            in_channels=out_channels,\n",
    "            out_channels=out_channels,\n",
    "            exp_r=exp_r,\n",
    "            kernel_size=kernel_size,\n",
    "            do_res=True,  # Can always do residual here since channels match\n",
    "            norm_type=norm_type,\n",
    "            dim=dim,\n",
    "            grn=grn\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.block1(x)\n",
    "        x = self.block2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SwinUNETR Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:32:48.622451Z",
     "iopub.status.busy": "2025-06-14T16:32:48.622061Z",
     "iopub.status.idle": "2025-06-14T16:32:48.711956Z",
     "shell.execute_reply": "2025-06-14T16:32:48.711265Z",
     "shell.execute_reply.started": "2025-06-14T16:32:48.622425Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "from collections.abc import Sequence\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from torch.nn import LayerNorm\n",
    "\n",
    "from monai.networks.blocks import MLPBlock as Mlp\n",
    "from monai.networks.blocks import PatchEmbed, UnetOutBlock, UnetrBasicBlock, UnetrUpBlock\n",
    "from monai.networks.layers import DropPath, trunc_normal_\n",
    "from monai.utils import ensure_tuple_rep, look_up_option, optional_import\n",
    "\n",
    "rearrange, _ = optional_import(\"einops\", name=\"rearrange\")\n",
    "\n",
    "class SwinUNETR(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        patch_size: int = 2,\n",
    "        depths: Sequence[int] = (2, 2, 2, 2),\n",
    "        num_heads: Sequence[int] = (3, 6, 12, 24),\n",
    "        window_size: Sequence[int] | int = 7,\n",
    "        qkv_bias: bool = True,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        feature_size: int = 24,\n",
    "        norm_name: tuple | str = \"instance\",\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        dropout_path_rate: float = 0.0,\n",
    "        normalize: bool = True,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        patch_norm: bool = False,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample: str | nn.Module = \"merging\",\n",
    "        use_v2: bool = False,\n",
    "        # MedNeXt specific parameters\n",
    "        mednext_exp_r: int = 4,\n",
    "        mednext_kernel_size: int = 7,\n",
    "        mednext_norm_type: str = 'group',\n",
    "        mednext_grn: bool = False\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_channels: dimension of input channels.\n",
    "            out_channels: dimension of output channels.\n",
    "            patch_size: size of the patch token.\n",
    "            feature_size: dimension of network feature size.\n",
    "            depths: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            window_size: local window size.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
    "            norm_name: feature normalization type and arguments.\n",
    "            drop_rate: dropout rate.\n",
    "            attn_drop_rate: attention dropout rate.\n",
    "            dropout_path_rate: drop path rate.\n",
    "            normalize: normalize output intermediate features in each stage.\n",
    "            norm_layer: normalization layer.\n",
    "            patch_norm: whether to apply normalization to the patch embedding. Default is False.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "            spatial_dims: number of spatial dims.\n",
    "            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n",
    "                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n",
    "                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n",
    "            use_v2: using swinunetr_v2, which adds a residual convolution block at the beggining of each swin stage.\n",
    "            \n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        if spatial_dims not in (2, 3):\n",
    "            raise ValueError(\"spatial dimension should be 2 or 3.\")\n",
    "\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "        patch_sizes = ensure_tuple_rep(self.patch_size, spatial_dims)\n",
    "        window_size = ensure_tuple_rep(window_size, spatial_dims)\n",
    "\n",
    "        if not (0 <= drop_rate <= 1):\n",
    "            raise ValueError(\"dropout rate should be between 0 and 1.\")\n",
    "\n",
    "        if not (0 <= attn_drop_rate <= 1):\n",
    "            raise ValueError(\"attention dropout rate should be between 0 and 1.\")\n",
    "\n",
    "        if not (0 <= dropout_path_rate <= 1):\n",
    "            raise ValueError(\"drop path rate should be between 0 and 1.\")\n",
    "\n",
    "        if feature_size % 12 != 0:\n",
    "            raise ValueError(\"feature_size should be divisible by 12.\")\n",
    "\n",
    "        self.normalize = normalize\n",
    "\n",
    "        self.swinViT = SwinTransformer(\n",
    "            in_chans=in_channels,\n",
    "            embed_dim=feature_size,\n",
    "            window_size=window_size,\n",
    "            patch_size=patch_sizes,\n",
    "            depths=depths,\n",
    "            num_heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            qkv_bias=qkv_bias,\n",
    "            drop_rate=drop_rate,\n",
    "            attn_drop_rate=attn_drop_rate,\n",
    "            drop_path_rate=dropout_path_rate,\n",
    "            norm_layer=norm_layer,\n",
    "            patch_norm=patch_norm,\n",
    "            use_checkpoint=use_checkpoint,\n",
    "            spatial_dims=spatial_dims,\n",
    "            downsample=look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample,\n",
    "            use_v2=use_v2,\n",
    "        )\n",
    "        \n",
    "        # MedNeXt-based Encoders\n",
    "        self.encoder1 = MedNeXtEncoder(\n",
    "            in_channels=in_channels,  # Original input channels (e.g., 4 for multi-modal MRI)\n",
    "            out_channels=feature_size,  # First level features (e.g., 48)\n",
    "            exp_r=mednext_exp_r,\n",
    "            kernel_size=mednext_kernel_size,\n",
    "            norm_type=mednext_norm_type,\n",
    "            spatial_dims=spatial_dims,\n",
    "            grn=mednext_grn\n",
    "        )\n",
    "    \n",
    "        self.encoder2 = MedNeXtEncoder(\n",
    "            in_channels=feature_size,  # Match SwinViT output channels\n",
    "            out_channels=feature_size,  # Keep same for skip connection compatibility\n",
    "            exp_r=mednext_exp_r,\n",
    "            kernel_size=mednext_kernel_size,\n",
    "            norm_type=mednext_norm_type,\n",
    "            spatial_dims=spatial_dims,\n",
    "            grn=mednext_grn\n",
    "        )\n",
    "    \n",
    "        self.encoder3 = MedNeXtEncoder(\n",
    "            in_channels=2 * feature_size,  # Match SwinViT output channels\n",
    "            out_channels=2 * feature_size,  # Keep same for skip connection compatibility\n",
    "            exp_r=mednext_exp_r,\n",
    "            kernel_size=mednext_kernel_size,\n",
    "            norm_type=mednext_norm_type,\n",
    "            spatial_dims=spatial_dims,\n",
    "            grn=mednext_grn\n",
    "        )\n",
    "    \n",
    "        self.encoder4 = MedNeXtEncoder(\n",
    "            in_channels=4 * feature_size,  # Match SwinViT output channels\n",
    "            out_channels=4 * feature_size,  # Keep same for skip connection compatibility\n",
    "            exp_r=mednext_exp_r,\n",
    "            kernel_size=mednext_kernel_size,\n",
    "            norm_type=mednext_norm_type,\n",
    "            spatial_dims=spatial_dims,\n",
    "            grn=mednext_grn\n",
    "        )\n",
    "    \n",
    "        self.encoder10 = MedNeXtEncoder(\n",
    "            in_channels=16 * feature_size,  # Match SwinViT bottleneck output\n",
    "            out_channels=16 * feature_size,\n",
    "            exp_r=mednext_exp_r,\n",
    "            kernel_size=mednext_kernel_size,\n",
    "            norm_type=mednext_norm_type,\n",
    "            spatial_dims=spatial_dims,\n",
    "            grn=mednext_grn\n",
    "        )\n",
    "    \n",
    "        # MedNeXt-based Decoders - these look correct\n",
    "        self.decoder5 = MedNeXtUpBlock(\n",
    "            in_channels=16 * feature_size,\n",
    "            out_channels=8 * feature_size,\n",
    "            exp_r=mednext_exp_r,\n",
    "            kernel_size=mednext_kernel_size,\n",
    "            norm_type=mednext_norm_type,\n",
    "            spatial_dims=spatial_dims,\n",
    "            grn=mednext_grn\n",
    "        )\n",
    "    \n",
    "        self.decoder4 = MedNeXtUpBlock(\n",
    "            in_channels=8 * feature_size,\n",
    "            out_channels=4 * feature_size,\n",
    "            exp_r=mednext_exp_r,\n",
    "            kernel_size=mednext_kernel_size,\n",
    "            norm_type=mednext_norm_type,\n",
    "            spatial_dims=spatial_dims,\n",
    "            grn=mednext_grn\n",
    "        )\n",
    "    \n",
    "        self.decoder3 = MedNeXtUpBlock(\n",
    "            in_channels=4 * feature_size,\n",
    "            out_channels=2 * feature_size,\n",
    "            exp_r=mednext_exp_r,\n",
    "            kernel_size=mednext_kernel_size,\n",
    "            norm_type=mednext_norm_type,\n",
    "            spatial_dims=spatial_dims,\n",
    "            grn=mednext_grn\n",
    "        )\n",
    "    \n",
    "        self.decoder2 = MedNeXtUpBlock(\n",
    "            in_channels=2 * feature_size,\n",
    "            out_channels=feature_size,\n",
    "            exp_r=mednext_exp_r,\n",
    "            kernel_size=mednext_kernel_size,\n",
    "            norm_type=mednext_norm_type,\n",
    "            spatial_dims=spatial_dims,\n",
    "            grn=mednext_grn\n",
    "        )\n",
    "    \n",
    "        self.decoder1 = MedNeXtUpBlock(\n",
    "            in_channels=feature_size,\n",
    "            out_channels=feature_size,\n",
    "            exp_r=mednext_exp_r,\n",
    "            kernel_size=mednext_kernel_size,\n",
    "            norm_type=mednext_norm_type,\n",
    "            spatial_dims=spatial_dims,\n",
    "            grn=mednext_grn\n",
    "        )\n",
    "    \n",
    "        # Output layer\n",
    "        self.out = UnetOutBlock(spatial_dims=spatial_dims, in_channels=feature_size, out_channels=out_channels)\n",
    "\n",
    "\n",
    "    def load_from(self, weights):\n",
    "        layers1_0: BasicLayer = self.swinViT.layers1[0]  # type: ignore[assignment]\n",
    "        layers2_0: BasicLayer = self.swinViT.layers2[0]  # type: ignore[assignment]\n",
    "        layers3_0: BasicLayer = self.swinViT.layers3[0]  # type: ignore[assignment]\n",
    "        layers4_0: BasicLayer = self.swinViT.layers4[0]  # type: ignore[assignment]\n",
    "        wstate = weights[\"state_dict\"]\n",
    "\n",
    "        with torch.no_grad():\n",
    "            self.swinViT.patch_embed.proj.weight.copy_(wstate[\"module.patch_embed.proj.weight\"])\n",
    "            self.swinViT.patch_embed.proj.bias.copy_(wstate[\"module.patch_embed.proj.bias\"])\n",
    "            for bname, block in layers1_0.blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers1\")  # type: ignore[operator]\n",
    "\n",
    "            if layers1_0.downsample is not None:\n",
    "                d = layers1_0.downsample\n",
    "                d.reduction.weight.copy_(wstate[\"module.layers1.0.downsample.reduction.weight\"])  # type: ignore\n",
    "                d.norm.weight.copy_(wstate[\"module.layers1.0.downsample.norm.weight\"])  # type: ignore\n",
    "                d.norm.bias.copy_(wstate[\"module.layers1.0.downsample.norm.bias\"])  # type: ignore\n",
    "\n",
    "            for bname, block in layers2_0.blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers2\")  # type: ignore[operator]\n",
    "\n",
    "            if layers2_0.downsample is not None:\n",
    "                d = layers2_0.downsample\n",
    "                d.reduction.weight.copy_(wstate[\"module.layers2.0.downsample.reduction.weight\"])  # type: ignore\n",
    "                d.norm.weight.copy_(wstate[\"module.layers2.0.downsample.norm.weight\"])  # type: ignore\n",
    "                d.norm.bias.copy_(wstate[\"module.layers2.0.downsample.norm.bias\"])  # type: ignore\n",
    "\n",
    "            for bname, block in layers3_0.blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers3\")  # type: ignore[operator]\n",
    "\n",
    "            if layers3_0.downsample is not None:\n",
    "                d = layers3_0.downsample\n",
    "                d.reduction.weight.copy_(wstate[\"module.layers3.0.downsample.reduction.weight\"])  # type: ignore\n",
    "                d.norm.weight.copy_(wstate[\"module.layers3.0.downsample.norm.weight\"])  # type: ignore\n",
    "                d.norm.bias.copy_(wstate[\"module.layers3.0.downsample.norm.bias\"])  # type: ignore\n",
    "\n",
    "            for bname, block in layers4_0.blocks.named_children():\n",
    "                block.load_from(weights, n_block=bname, layer=\"layers4\")  # type: ignore[operator]\n",
    "\n",
    "            if layers4_0.downsample is not None:\n",
    "                d = layers4_0.downsample\n",
    "                d.reduction.weight.copy_(wstate[\"module.layers4.0.downsample.reduction.weight\"])  # type: ignore\n",
    "                d.norm.weight.copy_(wstate[\"module.layers4.0.downsample.norm.weight\"])  # type: ignore\n",
    "                d.norm.bias.copy_(wstate[\"module.layers4.0.downsample.norm.bias\"])  # type: ignore\n",
    "\n",
    "    @torch.jit.unused\n",
    "    def _check_input_size(self, spatial_shape):\n",
    "        img_size = np.array(spatial_shape)\n",
    "        remainder = (img_size % np.power(self.patch_size, 5)) > 0\n",
    "        if remainder.any():\n",
    "            wrong_dims = (np.where(remainder)[0] + 2).tolist()\n",
    "            raise ValueError(\n",
    "                f\"spatial dimensions {wrong_dims} of input image (spatial shape: {spatial_shape})\"\n",
    "                f\" must be divisible by {self.patch_size}**5.\"\n",
    "            )\n",
    "\n",
    "    def forward(self, x_in):\n",
    "        if not torch.jit.is_scripting() and not torch.jit.is_tracing():\n",
    "            self._check_input_size(x_in.shape[2:])\n",
    "        hidden_states_out = self.swinViT(x_in, self.normalize)\n",
    "        enc0 = self.encoder1(x_in)\n",
    "        enc1 = self.encoder2(hidden_states_out[0])\n",
    "        enc2 = self.encoder3(hidden_states_out[1])\n",
    "        enc3 = self.encoder4(hidden_states_out[2])\n",
    "        dec4 = self.encoder10(hidden_states_out[4])\n",
    "        dec3 = self.decoder5(dec4, hidden_states_out[3])\n",
    "        dec2 = self.decoder4(dec3, enc3)\n",
    "        dec1 = self.decoder3(dec2, enc2)\n",
    "        dec0 = self.decoder2(dec1, enc1)\n",
    "        out = self.decoder1(dec0, enc0)\n",
    "        logits = self.out(out)\n",
    "        return logits\n",
    "\n",
    "\n",
    "def window_partition(x, window_size):\n",
    "    \"\"\"window partition operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        x: input tensor.\n",
    "        window_size: local window size.\n",
    "    \"\"\"\n",
    "    x_shape = x.size()  # length 4 or 5 only\n",
    "    if len(x_shape) == 5:\n",
    "        b, d, h, w, c = x_shape\n",
    "        x = x.view(\n",
    "            b,\n",
    "            d // window_size[0],\n",
    "            window_size[0],\n",
    "            h // window_size[1],\n",
    "            window_size[1],\n",
    "            w // window_size[2],\n",
    "            window_size[2],\n",
    "            c,\n",
    "        )\n",
    "        windows = (\n",
    "            x.permute(0, 1, 3, 5, 2, 4, 6, 7).contiguous().view(-1, window_size[0] * window_size[1] * window_size[2], c)\n",
    "        )\n",
    "    else:  # if len(x_shape) == 4:\n",
    "        b, h, w, c = x.shape\n",
    "        x = x.view(b, h // window_size[0], window_size[0], w // window_size[1], window_size[1], c)\n",
    "        windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size[0] * window_size[1], c)\n",
    "\n",
    "    return windows\n",
    "\n",
    "\n",
    "def window_reverse(windows, window_size, dims):\n",
    "    \"\"\"window reverse operation based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        windows: windows tensor.\n",
    "        window_size: local window size.\n",
    "        dims: dimension values.\n",
    "    \"\"\"\n",
    "    if len(dims) == 4:\n",
    "        b, d, h, w = dims\n",
    "        x = windows.view(\n",
    "            b,\n",
    "            d // window_size[0],\n",
    "            h // window_size[1],\n",
    "            w // window_size[2],\n",
    "            window_size[0],\n",
    "            window_size[1],\n",
    "            window_size[2],\n",
    "            -1,\n",
    "        )\n",
    "        x = x.permute(0, 1, 4, 2, 5, 3, 6, 7).contiguous().view(b, d, h, w, -1)\n",
    "\n",
    "    elif len(dims) == 3:\n",
    "        b, h, w = dims\n",
    "        x = windows.view(b, h // window_size[0], w // window_size[1], window_size[0], window_size[1], -1)\n",
    "        x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(b, h, w, -1)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_window_size(x_size, window_size, shift_size=None):\n",
    "    \"\"\"Computing window size based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        x_size: input size.\n",
    "        window_size: local window size.\n",
    "        shift_size: window shifting size.\n",
    "    \"\"\"\n",
    "\n",
    "    use_window_size = list(window_size)\n",
    "    if shift_size is not None:\n",
    "        use_shift_size = list(shift_size)\n",
    "    for i in range(len(x_size)):\n",
    "        if x_size[i] <= window_size[i]:\n",
    "            use_window_size[i] = x_size[i]\n",
    "            if shift_size is not None:\n",
    "                use_shift_size[i] = 0\n",
    "\n",
    "    if shift_size is None:\n",
    "        return tuple(use_window_size)\n",
    "    else:\n",
    "        return tuple(use_window_size), tuple(use_shift_size)\n",
    "\n",
    "\n",
    "class WindowAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Window based multi-head self attention module with relative position bias based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        qkv_bias: bool = False,\n",
    "        attn_drop: float = 0.0,\n",
    "        proj_drop: float = 0.0,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            num_heads: number of attention heads.\n",
    "            window_size: local window size.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            attn_drop: attention dropout rate.\n",
    "            proj_drop: dropout rate of output.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.window_size = window_size\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim**-0.5\n",
    "        mesh_args = torch.meshgrid.__kwdefaults__\n",
    "\n",
    "        if len(self.window_size) == 3:\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros(\n",
    "                    (2 * self.window_size[0] - 1) * (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1),\n",
    "                    num_heads,\n",
    "                )\n",
    "            )\n",
    "            coords_d = torch.arange(self.window_size[0])\n",
    "            coords_h = torch.arange(self.window_size[1])\n",
    "            coords_w = torch.arange(self.window_size[2])\n",
    "            if mesh_args is not None:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w, indexing=\"ij\"))\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(coords_d, coords_h, coords_w))\n",
    "            coords_flatten = torch.flatten(coords, 1)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 2] += self.window_size[2] - 1\n",
    "            relative_coords[:, :, 0] *= (2 * self.window_size[1] - 1) * (2 * self.window_size[2] - 1)\n",
    "            relative_coords[:, :, 1] *= 2 * self.window_size[2] - 1\n",
    "        elif len(self.window_size) == 2:\n",
    "            self.relative_position_bias_table = nn.Parameter(\n",
    "                torch.zeros((2 * window_size[0] - 1) * (2 * window_size[1] - 1), num_heads)\n",
    "            )\n",
    "            coords_h = torch.arange(self.window_size[0])\n",
    "            coords_w = torch.arange(self.window_size[1])\n",
    "            if mesh_args is not None:\n",
    "                coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n",
    "            else:\n",
    "                coords = torch.stack(torch.meshgrid(coords_h, coords_w))\n",
    "            coords_flatten = torch.flatten(coords, 1)\n",
    "            relative_coords = coords_flatten[:, :, None] - coords_flatten[:, None, :]\n",
    "            relative_coords = relative_coords.permute(1, 2, 0).contiguous()\n",
    "            relative_coords[:, :, 0] += self.window_size[0] - 1\n",
    "            relative_coords[:, :, 1] += self.window_size[1] - 1\n",
    "            relative_coords[:, :, 0] *= 2 * self.window_size[1] - 1\n",
    "\n",
    "        relative_position_index = relative_coords.sum(-1)\n",
    "        self.register_buffer(\"relative_position_index\", relative_position_index)\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "        trunc_normal_(self.relative_position_bias_table, std=0.02)\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        b, n, c = x.shape\n",
    "        qkv = self.qkv(x).reshape(b, n, 3, self.num_heads, c // self.num_heads).permute(2, 0, 3, 1, 4)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "        q = q * self.scale\n",
    "        attn = q @ k.transpose(-2, -1)\n",
    "        relative_position_bias = self.relative_position_bias_table[\n",
    "            self.relative_position_index.clone()[:n, :n].reshape(-1)  # type: ignore[operator]\n",
    "        ].reshape(n, n, -1)\n",
    "        relative_position_bias = relative_position_bias.permute(2, 0, 1).contiguous()\n",
    "        attn = attn + relative_position_bias.unsqueeze(0)\n",
    "        if mask is not None:\n",
    "            nw = mask.shape[0]\n",
    "            attn = attn.view(b // nw, nw, self.num_heads, n, n) + mask.unsqueeze(1).unsqueeze(0)\n",
    "            attn = attn.view(-1, self.num_heads, n, n)\n",
    "            attn = self.softmax(attn)\n",
    "        else:\n",
    "            attn = self.softmax(attn)\n",
    "\n",
    "        attn = self.attn_drop(attn).to(v.dtype)\n",
    "        x = (attn @ v).transpose(1, 2).reshape(b, n, c)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformerBlock(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer block based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        shift_size: Sequence[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        drop_path: float = 0.0,\n",
    "        act_layer: str = \"GELU\",\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        use_checkpoint: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            num_heads: number of attention heads.\n",
    "            window_size: local window size.\n",
    "            shift_size: window shift size.\n",
    "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            drop: dropout rate.\n",
    "            attn_drop: attention dropout rate.\n",
    "            drop_path: stochastic depth rate.\n",
    "            act_layer: activation layer.\n",
    "            norm_layer: normalization layer.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        self.num_heads = num_heads\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = shift_size\n",
    "        self.mlp_ratio = mlp_ratio\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.norm1 = norm_layer(dim)\n",
    "        self.attn = WindowAttention(\n",
    "            dim,\n",
    "            window_size=self.window_size,\n",
    "            num_heads=num_heads,\n",
    "            qkv_bias=qkv_bias,\n",
    "            attn_drop=attn_drop,\n",
    "            proj_drop=drop,\n",
    "        )\n",
    "\n",
    "        self.drop_path = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
    "        self.norm2 = norm_layer(dim)\n",
    "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
    "        self.mlp = Mlp(hidden_size=dim, mlp_dim=mlp_hidden_dim, act=act_layer, dropout_rate=drop, dropout_mode=\"swin\")\n",
    "\n",
    "    def forward_part1(self, x, mask_matrix):\n",
    "        x_shape = x.size()\n",
    "        x = self.norm1(x)\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x.shape\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "            pad_l = pad_t = pad_d0 = 0\n",
    "            pad_d1 = (window_size[0] - d % window_size[0]) % window_size[0]\n",
    "            pad_b = (window_size[1] - h % window_size[1]) % window_size[1]\n",
    "            pad_r = (window_size[2] - w % window_size[2]) % window_size[2]\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b, pad_d0, pad_d1))\n",
    "            _, dp, hp, wp, _ = x.shape\n",
    "            dims = [b, dp, hp, wp]\n",
    "\n",
    "        else:  # elif len(x_shape) == 4\n",
    "            b, h, w, c = x.shape\n",
    "            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n",
    "            pad_l = pad_t = 0\n",
    "            pad_b = (window_size[0] - h % window_size[0]) % window_size[0]\n",
    "            pad_r = (window_size[1] - w % window_size[1]) % window_size[1]\n",
    "            x = F.pad(x, (0, 0, pad_l, pad_r, pad_t, pad_b))\n",
    "            _, hp, wp, _ = x.shape\n",
    "            dims = [b, hp, wp]\n",
    "\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            if len(x_shape) == 5:\n",
    "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1], -shift_size[2]), dims=(1, 2, 3))\n",
    "            elif len(x_shape) == 4:\n",
    "                shifted_x = torch.roll(x, shifts=(-shift_size[0], -shift_size[1]), dims=(1, 2))\n",
    "            attn_mask = mask_matrix\n",
    "        else:\n",
    "            shifted_x = x\n",
    "            attn_mask = None\n",
    "        x_windows = window_partition(shifted_x, window_size)\n",
    "        attn_windows = self.attn(x_windows, mask=attn_mask)\n",
    "        attn_windows = attn_windows.view(-1, *(window_size + (c,)))\n",
    "        shifted_x = window_reverse(attn_windows, window_size, dims)\n",
    "        if any(i > 0 for i in shift_size):\n",
    "            if len(x_shape) == 5:\n",
    "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1], shift_size[2]), dims=(1, 2, 3))\n",
    "            elif len(x_shape) == 4:\n",
    "                x = torch.roll(shifted_x, shifts=(shift_size[0], shift_size[1]), dims=(1, 2))\n",
    "        else:\n",
    "            x = shifted_x\n",
    "\n",
    "        if len(x_shape) == 5:\n",
    "            if pad_d1 > 0 or pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :d, :h, :w, :].contiguous()\n",
    "        elif len(x_shape) == 4:\n",
    "            if pad_r > 0 or pad_b > 0:\n",
    "                x = x[:, :h, :w, :].contiguous()\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward_part2(self, x):\n",
    "        return self.drop_path(self.mlp(self.norm2(x)))\n",
    "\n",
    "    def load_from(self, weights, n_block, layer):\n",
    "        root = f\"module.{layer}.0.blocks.{n_block}.\"\n",
    "        block_names = [\n",
    "            \"norm1.weight\",\n",
    "            \"norm1.bias\",\n",
    "            \"attn.relative_position_bias_table\",\n",
    "            \"attn.relative_position_index\",\n",
    "            \"attn.qkv.weight\",\n",
    "            \"attn.qkv.bias\",\n",
    "            \"attn.proj.weight\",\n",
    "            \"attn.proj.bias\",\n",
    "            \"norm2.weight\",\n",
    "            \"norm2.bias\",\n",
    "            \"mlp.fc1.weight\",\n",
    "            \"mlp.fc1.bias\",\n",
    "            \"mlp.fc2.weight\",\n",
    "            \"mlp.fc2.bias\",\n",
    "        ]\n",
    "        with torch.no_grad():\n",
    "            self.norm1.weight.copy_(weights[\"state_dict\"][root + block_names[0]])\n",
    "            self.norm1.bias.copy_(weights[\"state_dict\"][root + block_names[1]])\n",
    "            self.attn.relative_position_bias_table.copy_(weights[\"state_dict\"][root + block_names[2]])\n",
    "            self.attn.relative_position_index.copy_(weights[\"state_dict\"][root + block_names[3]])  # type: ignore[operator]\n",
    "            self.attn.qkv.weight.copy_(weights[\"state_dict\"][root + block_names[4]])\n",
    "            self.attn.qkv.bias.copy_(weights[\"state_dict\"][root + block_names[5]])\n",
    "            self.attn.proj.weight.copy_(weights[\"state_dict\"][root + block_names[6]])\n",
    "            self.attn.proj.bias.copy_(weights[\"state_dict\"][root + block_names[7]])\n",
    "            self.norm2.weight.copy_(weights[\"state_dict\"][root + block_names[8]])\n",
    "            self.norm2.bias.copy_(weights[\"state_dict\"][root + block_names[9]])\n",
    "            self.mlp.linear1.weight.copy_(weights[\"state_dict\"][root + block_names[10]])\n",
    "            self.mlp.linear1.bias.copy_(weights[\"state_dict\"][root + block_names[11]])\n",
    "            self.mlp.linear2.weight.copy_(weights[\"state_dict\"][root + block_names[12]])\n",
    "            self.mlp.linear2.bias.copy_(weights[\"state_dict\"][root + block_names[13]])\n",
    "\n",
    "    def forward(self, x, mask_matrix):\n",
    "        shortcut = x\n",
    "        if self.use_checkpoint:\n",
    "            x = checkpoint.checkpoint(self.forward_part1, x, mask_matrix, use_reentrant=False)\n",
    "        else:\n",
    "            x = self.forward_part1(x, mask_matrix)\n",
    "        x = shortcut + self.drop_path(x)\n",
    "        if self.use_checkpoint:\n",
    "            x = x + checkpoint.checkpoint(self.forward_part2, x, use_reentrant=False)\n",
    "        else:\n",
    "            x = x + self.forward_part2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMergingV2(nn.Module):\n",
    "    \"\"\"\n",
    "    Patch merging layer based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim: int, norm_layer: type[LayerNorm] = nn.LayerNorm, spatial_dims: int = 3) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            norm_layer: normalization layer.\n",
    "            spatial_dims: number of spatial dims.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.dim = dim\n",
    "        if spatial_dims == 3:\n",
    "            self.reduction = nn.Linear(8 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(8 * dim)\n",
    "        elif spatial_dims == 2:\n",
    "            self.reduction = nn.Linear(4 * dim, 2 * dim, bias=False)\n",
    "            self.norm = norm_layer(4 * dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, d, h, w, c = x_shape\n",
    "            pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
    "            x = torch.cat(\n",
    "                [x[:, i::2, j::2, k::2, :] for i, j, k in itertools.product(range(2), range(2), range(2))], -1\n",
    "            )\n",
    "\n",
    "        elif len(x_shape) == 4:\n",
    "            b, h, w, c = x_shape\n",
    "            pad_input = (h % 2 == 1) or (w % 2 == 1)\n",
    "            if pad_input:\n",
    "                x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2))\n",
    "            x = torch.cat([x[:, j::2, i::2, :] for i, j in itertools.product(range(2), range(2))], -1)\n",
    "\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class PatchMerging(PatchMergingV2):\n",
    "    \"\"\"The `PatchMerging` module previously defined in v0.9.0.\"\"\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 4:\n",
    "            return super().forward(x)\n",
    "        if len(x_shape) != 5:\n",
    "            raise ValueError(f\"expecting 5D x, got {x.shape}.\")\n",
    "        b, d, h, w, c = x_shape\n",
    "        pad_input = (h % 2 == 1) or (w % 2 == 1) or (d % 2 == 1)\n",
    "        if pad_input:\n",
    "            x = F.pad(x, (0, 0, 0, w % 2, 0, h % 2, 0, d % 2))\n",
    "        x0 = x[:, 0::2, 0::2, 0::2, :]\n",
    "        x1 = x[:, 1::2, 0::2, 0::2, :]\n",
    "        x2 = x[:, 0::2, 1::2, 0::2, :]\n",
    "        x3 = x[:, 0::2, 0::2, 1::2, :]\n",
    "        x4 = x[:, 1::2, 1::2, 0::2, :]\n",
    "        x5 = x[:, 1::2, 0::2, 1::2, :]\n",
    "        x6 = x[:, 0::2, 1::2, 1::2, :]\n",
    "        x7 = x[:, 1::2, 1::2, 1::2, :]\n",
    "        x = torch.cat([x0, x1, x2, x3, x4, x5, x6, x7], -1)\n",
    "        x = self.norm(x)\n",
    "        x = self.reduction(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "MERGING_MODE = {\"merging\": PatchMerging, \"mergingv2\": PatchMergingV2}\n",
    "\n",
    "\n",
    "def compute_mask(dims, window_size, shift_size, device):\n",
    "    \"\"\"Computing region masks based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "\n",
    "     Args:\n",
    "        dims: dimension values.\n",
    "        window_size: local window size.\n",
    "        shift_size: shift size.\n",
    "        device: device.\n",
    "    \"\"\"\n",
    "\n",
    "    cnt = 0\n",
    "\n",
    "    if len(dims) == 3:\n",
    "        d, h, w = dims\n",
    "        img_mask = torch.zeros((1, d, h, w, 1), device=device)\n",
    "        for d in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for h in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                for w in slice(-window_size[2]), slice(-window_size[2], -shift_size[2]), slice(-shift_size[2], None):\n",
    "                    img_mask[:, d, h, w, :] = cnt\n",
    "                    cnt += 1\n",
    "\n",
    "    elif len(dims) == 2:\n",
    "        h, w = dims\n",
    "        img_mask = torch.zeros((1, h, w, 1), device=device)\n",
    "        for h in slice(-window_size[0]), slice(-window_size[0], -shift_size[0]), slice(-shift_size[0], None):\n",
    "            for w in slice(-window_size[1]), slice(-window_size[1], -shift_size[1]), slice(-shift_size[1], None):\n",
    "                img_mask[:, h, w, :] = cnt\n",
    "                cnt += 1\n",
    "\n",
    "    mask_windows = window_partition(img_mask, window_size)\n",
    "    mask_windows = mask_windows.squeeze(-1)\n",
    "    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n",
    "    attn_mask = attn_mask.masked_fill(attn_mask != 0, float(-100.0)).masked_fill(attn_mask == 0, float(0.0))\n",
    "\n",
    "    return attn_mask\n",
    "\n",
    "\n",
    "class BasicLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Basic Swin Transformer layer in one stage based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim: int,\n",
    "        depth: int,\n",
    "        num_heads: int,\n",
    "        window_size: Sequence[int],\n",
    "        drop_path: list,\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = False,\n",
    "        drop: float = 0.0,\n",
    "        attn_drop: float = 0.0,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        downsample: nn.Module | None = None,\n",
    "        use_checkpoint: bool = False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dim: number of feature channels.\n",
    "            depth: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            window_size: local window size.\n",
    "            drop_path: stochastic depth rate.\n",
    "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            drop: dropout rate.\n",
    "            attn_drop: attention dropout rate.\n",
    "            norm_layer: normalization layer.\n",
    "            downsample: an optional downsampling layer at the end of the layer.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.shift_size = tuple(i // 2 for i in window_size)\n",
    "        self.no_shift = tuple(0 for i in window_size)\n",
    "        self.depth = depth\n",
    "        self.use_checkpoint = use_checkpoint\n",
    "        self.blocks = nn.ModuleList(\n",
    "            [\n",
    "                SwinTransformerBlock(\n",
    "                    dim=dim,\n",
    "                    num_heads=num_heads,\n",
    "                    window_size=self.window_size,\n",
    "                    shift_size=self.no_shift if (i % 2 == 0) else self.shift_size,\n",
    "                    mlp_ratio=mlp_ratio,\n",
    "                    qkv_bias=qkv_bias,\n",
    "                    drop=drop,\n",
    "                    attn_drop=attn_drop,\n",
    "                    drop_path=drop_path[i] if isinstance(drop_path, list) else drop_path,\n",
    "                    norm_layer=norm_layer,\n",
    "                    use_checkpoint=use_checkpoint,\n",
    "                )\n",
    "                for i in range(depth)\n",
    "            ]\n",
    "        )\n",
    "        self.downsample = downsample\n",
    "        if callable(self.downsample):\n",
    "            self.downsample = downsample(dim=dim, norm_layer=norm_layer, spatial_dims=len(self.window_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_shape = x.size()\n",
    "        if len(x_shape) == 5:\n",
    "            b, c, d, h, w = x_shape\n",
    "            window_size, shift_size = get_window_size((d, h, w), self.window_size, self.shift_size)\n",
    "            x = rearrange(x, \"b c d h w -> b d h w c\")\n",
    "            dp = int(np.ceil(d / window_size[0])) * window_size[0]\n",
    "            hp = int(np.ceil(h / window_size[1])) * window_size[1]\n",
    "            wp = int(np.ceil(w / window_size[2])) * window_size[2]\n",
    "            attn_mask = compute_mask([dp, hp, wp], window_size, shift_size, x.device)\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask)\n",
    "            x = x.view(b, d, h, w, -1)\n",
    "            if self.downsample is not None:\n",
    "                x = self.downsample(x)\n",
    "            x = rearrange(x, \"b d h w c -> b c d h w\")\n",
    "\n",
    "        elif len(x_shape) == 4:\n",
    "            b, c, h, w = x_shape\n",
    "            window_size, shift_size = get_window_size((h, w), self.window_size, self.shift_size)\n",
    "            x = rearrange(x, \"b c h w -> b h w c\")\n",
    "            hp = int(np.ceil(h / window_size[0])) * window_size[0]\n",
    "            wp = int(np.ceil(w / window_size[1])) * window_size[1]\n",
    "            attn_mask = compute_mask([hp, wp], window_size, shift_size, x.device)\n",
    "            for blk in self.blocks:\n",
    "                x = blk(x, attn_mask)\n",
    "            x = x.view(b, h, w, -1)\n",
    "            if self.downsample is not None:\n",
    "                x = self.downsample(x)\n",
    "            x = rearrange(x, \"b h w c -> b c h w\")\n",
    "        return x\n",
    "\n",
    "\n",
    "class SwinTransformer(nn.Module):\n",
    "    \"\"\"\n",
    "    Swin Transformer based on: \"Liu et al.,\n",
    "    Swin Transformer: Hierarchical Vision Transformer using Shifted Windows\n",
    "    <https://arxiv.org/abs/2103.14030>\"\n",
    "    https://github.com/microsoft/Swin-Transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_chans: int,\n",
    "        embed_dim: int,\n",
    "        window_size: Sequence[int],\n",
    "        patch_size: Sequence[int],\n",
    "        depths: Sequence[int],\n",
    "        num_heads: Sequence[int],\n",
    "        mlp_ratio: float = 4.0,\n",
    "        qkv_bias: bool = True,\n",
    "        drop_rate: float = 0.0,\n",
    "        attn_drop_rate: float = 0.0,\n",
    "        drop_path_rate: float = 0.0,\n",
    "        norm_layer: type[LayerNorm] = nn.LayerNorm,\n",
    "        patch_norm: bool = False,\n",
    "        use_checkpoint: bool = False,\n",
    "        spatial_dims: int = 3,\n",
    "        downsample=\"merging\",\n",
    "        use_v2=False,\n",
    "    ) -> None:\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            in_chans: dimension of input channels.\n",
    "            embed_dim: number of linear projection output channels.\n",
    "            window_size: local window size.\n",
    "            patch_size: patch size.\n",
    "            depths: number of layers in each stage.\n",
    "            num_heads: number of attention heads.\n",
    "            mlp_ratio: ratio of mlp hidden dim to embedding dim.\n",
    "            qkv_bias: add a learnable bias to query, key, value.\n",
    "            drop_rate: dropout rate.\n",
    "            attn_drop_rate: attention dropout rate.\n",
    "            drop_path_rate: stochastic depth rate.\n",
    "            norm_layer: normalization layer.\n",
    "            patch_norm: add normalization after patch embedding.\n",
    "            use_checkpoint: use gradient checkpointing for reduced memory usage.\n",
    "            spatial_dims: spatial dimension.\n",
    "            downsample: module used for downsampling, available options are `\"mergingv2\"`, `\"merging\"` and a\n",
    "                user-specified `nn.Module` following the API defined in :py:class:`monai.networks.nets.PatchMerging`.\n",
    "                The default is currently `\"merging\"` (the original version defined in v0.9.0).\n",
    "            use_v2: using swinunetr_v2, which adds a residual convolution block at the beginning of each swin stage.\n",
    "        \"\"\"\n",
    "\n",
    "        super().__init__()\n",
    "        self.num_layers = len(depths)\n",
    "        self.embed_dim = embed_dim\n",
    "        self.patch_norm = patch_norm\n",
    "        self.window_size = window_size\n",
    "        self.patch_size = patch_size\n",
    "        self.patch_embed = PatchEmbed(\n",
    "            patch_size=self.patch_size,\n",
    "            in_chans=in_chans,\n",
    "            embed_dim=embed_dim,\n",
    "            norm_layer=norm_layer if self.patch_norm else None,  # type: ignore\n",
    "            spatial_dims=spatial_dims,\n",
    "        )\n",
    "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
    "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, sum(depths))]\n",
    "        self.use_v2 = use_v2\n",
    "        self.layers1 = nn.ModuleList()\n",
    "        self.layers2 = nn.ModuleList()\n",
    "        self.layers3 = nn.ModuleList()\n",
    "        self.layers4 = nn.ModuleList()\n",
    "        if self.use_v2:\n",
    "            self.layers1c = nn.ModuleList()\n",
    "            self.layers2c = nn.ModuleList()\n",
    "            self.layers3c = nn.ModuleList()\n",
    "            self.layers4c = nn.ModuleList()\n",
    "        down_sample_mod = look_up_option(downsample, MERGING_MODE) if isinstance(downsample, str) else downsample\n",
    "        for i_layer in range(self.num_layers):\n",
    "            layer = BasicLayer(\n",
    "                dim=int(embed_dim * 2**i_layer),\n",
    "                depth=depths[i_layer],\n",
    "                num_heads=num_heads[i_layer],\n",
    "                window_size=self.window_size,\n",
    "                drop_path=dpr[sum(depths[:i_layer]) : sum(depths[: i_layer + 1])],\n",
    "                mlp_ratio=mlp_ratio,\n",
    "                qkv_bias=qkv_bias,\n",
    "                drop=drop_rate,\n",
    "                attn_drop=attn_drop_rate,\n",
    "                norm_layer=norm_layer,\n",
    "                downsample=down_sample_mod,\n",
    "                use_checkpoint=use_checkpoint,\n",
    "            )\n",
    "            if i_layer == 0:\n",
    "                self.layers1.append(layer)\n",
    "            elif i_layer == 1:\n",
    "                self.layers2.append(layer)\n",
    "            elif i_layer == 2:\n",
    "                self.layers3.append(layer)\n",
    "            elif i_layer == 3:\n",
    "                self.layers4.append(layer)\n",
    "            if self.use_v2:\n",
    "                layerc = UnetrBasicBlock(\n",
    "                    spatial_dims=spatial_dims,\n",
    "                    in_channels=embed_dim * 2**i_layer,\n",
    "                    out_channels=embed_dim * 2**i_layer,\n",
    "                    kernel_size=3,\n",
    "                    stride=1,\n",
    "                    norm_name=\"instance\",\n",
    "                    res_block=True,\n",
    "                )\n",
    "                if i_layer == 0:\n",
    "                    self.layers1c.append(layerc)\n",
    "                elif i_layer == 1:\n",
    "                    self.layers2c.append(layerc)\n",
    "                elif i_layer == 2:\n",
    "                    self.layers3c.append(layerc)\n",
    "                elif i_layer == 3:\n",
    "                    self.layers4c.append(layerc)\n",
    "\n",
    "        self.num_features = int(embed_dim * 2 ** (self.num_layers - 1))\n",
    "\n",
    "    def proj_out(self, x, normalize=False):\n",
    "        if normalize:\n",
    "            x_shape = x.shape\n",
    "            # Force trace() to generate a constant by casting to int\n",
    "            ch = int(x_shape[1])\n",
    "            if len(x_shape) == 5:\n",
    "                x = rearrange(x, \"n c d h w -> n d h w c\")\n",
    "                x = F.layer_norm(x, [ch])\n",
    "                x = rearrange(x, \"n d h w c -> n c d h w\")\n",
    "            elif len(x_shape) == 4:\n",
    "                x = rearrange(x, \"n c h w -> n h w c\")\n",
    "                x = F.layer_norm(x, [ch])\n",
    "                x = rearrange(x, \"n h w c -> n c h w\")\n",
    "        return x\n",
    "\n",
    "    def forward(self, x, normalize=True):\n",
    "        x0 = self.patch_embed(x)\n",
    "        x0 = self.pos_drop(x0)\n",
    "        x0_out = self.proj_out(x0, normalize)\n",
    "        if self.use_v2:\n",
    "            x0 = self.layers1c[0](x0.contiguous())\n",
    "        x1 = self.layers1[0](x0.contiguous())\n",
    "        x1_out = self.proj_out(x1, normalize)\n",
    "        if self.use_v2:\n",
    "            x1 = self.layers2c[0](x1.contiguous())\n",
    "        x2 = self.layers2[0](x1.contiguous())\n",
    "        x2_out = self.proj_out(x2, normalize)\n",
    "        if self.use_v2:\n",
    "            x2 = self.layers3c[0](x2.contiguous())\n",
    "        x3 = self.layers3[0](x2.contiguous())\n",
    "        x3_out = self.proj_out(x3, normalize)\n",
    "        if self.use_v2:\n",
    "            x3 = self.layers4c[0](x3.contiguous())\n",
    "        x4 = self.layers4[0](x3.contiguous())\n",
    "        x4_out = self.proj_out(x4, normalize)\n",
    "        return [x0_out, x1_out, x2_out, x3_out, x4_out]\n",
    "\n",
    "\n",
    "def filter_swinunetr(key, value):\n",
    "    \"\"\"\n",
    "    A filter function used to filter the pretrained weights from [1], then the weights can be loaded into MONAI SwinUNETR Model.\n",
    "    This function is typically used with `monai.networks.copy_model_state`\n",
    "    [1] \"Valanarasu JM et al., Disruptive Autoencoders: Leveraging Low-level features for 3D Medical Image Pre-training\n",
    "    <https://arxiv.org/abs/2307.16896>\"\n",
    "\n",
    "    Args:\n",
    "        key: the key in the source state dict used for the update.\n",
    "        value: the value in the source state dict used for the update.\n",
    "\n",
    "    Examples::\n",
    "\n",
    "        import torch\n",
    "        from monai.apps import download_url\n",
    "        from monai.networks.utils import copy_model_state\n",
    "        from monai.networks.nets.swin_unetr import SwinUNETR, filter_swinunetr\n",
    "\n",
    "        model = SwinUNETR(in_channels=1, out_channels=3, feature_size=48)\n",
    "        resource = (\n",
    "            \"https://github.com/Project-MONAI/MONAI-extra-test-data/releases/download/0.8.1/ssl_pretrained_weights.pth\"\n",
    "        )\n",
    "        ssl_weights_path = \"./ssl_pretrained_weights.pth\"\n",
    "        download_url(resource, ssl_weights_path)\n",
    "        ssl_weights = torch.load(ssl_weights_path, weights_only=True)[\"model\"]\n",
    "\n",
    "        dst_dict, loaded, not_loaded = copy_model_state(model, ssl_weights, filter_func=filter_swinunetr)\n",
    "\n",
    "    \"\"\"\n",
    "    if key in [\n",
    "        \"encoder.mask_token\",\n",
    "        \"encoder.norm.weight\",\n",
    "        \"encoder.norm.bias\",\n",
    "        \"out.conv.conv.weight\",\n",
    "        \"out.conv.conv.bias\",\n",
    "    ]:\n",
    "        return None\n",
    "\n",
    "    if key[:8] == \"encoder.\":\n",
    "        if key[8:19] == \"patch_embed\":\n",
    "            new_key = \"swinViT.\" + key[8:]\n",
    "        else:\n",
    "            new_key = \"swinViT.\" + key[8:18] + key[20:]\n",
    "\n",
    "        return new_key, value\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:43:11.667061Z",
     "iopub.status.busy": "2025-06-14T16:43:11.666717Z",
     "iopub.status.idle": "2025-06-14T16:43:11.693365Z",
     "shell.execute_reply": "2025-06-14T16:43:11.692481Z",
     "shell.execute_reply.started": "2025-06-14T16:43:11.667037Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "from monai.losses import DiceLoss, DiceCELoss, FocalLoss\n",
    "from monai.metrics import DiceMetric\n",
    "from monai.transforms import Compose, Activations, AsDiscrete\n",
    "from monai.data import PersistentDataset, list_data_collate, decollate_batch, DataLoader, load_decathlon_datalist, CacheDataset\n",
    "from monai.inferers import sliding_window_inference\n",
    "from torch.optim import Adam, AdamW\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "from monai.data import DataLoader, Dataset\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks.timer import Timer\n",
    "from torch.cuda.amp import GradScaler\n",
    "import wandb\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "import math\n",
    "\n",
    "class BrainTumorSegmentation(pl.LightningModule):\n",
    "    def __init__(self, train_loader, val_loader, max_epochs=100,\n",
    "                 val_interval=1, learning_rate=1e-4, feature_size=48,\n",
    "                 weight_decay=1e-5, warmup_epochs=10, roi_size=(96, 96, 96),\n",
    "                 sw_batch_size=2, use_v2=True, depths=(2, 2, 2, 2),\n",
    "                 num_heads=(3, 6, 12, 24), downsample=\"mergingv2\",\n",
    "                 use_class_weights=True,\n",
    "                 ## MedNext Params ##\n",
    "                 mednext_exp_r=4, mednext_kernel_size=7,\n",
    "                 mednext_norm_type='group', mednext_grn=True,\n",
    "                 ):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.save_hyperparameters()\n",
    "        \n",
    "        # Base SwinUNETR model\n",
    "        self.model = SwinUNETR(\n",
    "            in_channels=4,\n",
    "            out_channels=3,\n",
    "            feature_size=self.hparams.feature_size,\n",
    "            use_checkpoint=True,\n",
    "            use_v2=self.hparams.use_v2,\n",
    "            spatial_dims=3,\n",
    "            depths=self.hparams.depths,\n",
    "            num_heads=self.hparams.num_heads,\n",
    "            norm_name=\"instance\",\n",
    "            drop_rate=0.0,\n",
    "            attn_drop_rate=0.0,\n",
    "            dropout_path_rate=0.0,\n",
    "            downsample=self.hparams.downsample,\n",
    "            # MedNext Params\n",
    "            mednext_exp_r = self.hparams.mednext_exp_r,\n",
    "            mednext_kernel_size = self.hparams.mednext_kernel_size,\n",
    "            mednext_norm_type = self.hparams.mednext_norm_type,\n",
    "            mednext_grn = self.hparams.mednext_grn,\n",
    "            \n",
    "        )\n",
    "        \n",
    "        # Class weights based on BraTS imbalance: ET (most rare) > TC > WT\n",
    "        if self.hparams.use_class_weights:\n",
    "            # Higher weights for more imbalanced classes\n",
    "            class_weights = torch.tensor([1.0, 3.0, 5.0])  # Background, WT, TC, ET\n",
    "        else:\n",
    "            class_weights = None\n",
    "            \n",
    "        # Loss functions with class weighting\n",
    "        self.dice_loss = DiceLoss(\n",
    "            smooth_nr=0, smooth_dr=1e-5, squared_pred=True, \n",
    "            to_onehot_y=False, sigmoid=True\n",
    "        )\n",
    "        self.ce_loss = DiceCELoss(\n",
    "            smooth_nr=0, smooth_dr=1e-5, squared_pred=True, \n",
    "            to_onehot_y=False, sigmoid=True\n",
    "        )\n",
    "        self.focal_loss = FocalLoss(\n",
    "            gamma=2.0, weight=class_weights, reduction='mean'\n",
    "        )\n",
    "        \n",
    "        # Standard Dice Loss Metrics\n",
    "        self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "        self.dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "        self.post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "        \n",
    "        self.best_metric = -1\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "\n",
    "        # Training metrics\n",
    "        self.avg_train_loss_values = []\n",
    "        self.train_loss_values = []\n",
    "        self.train_metric_values = []\n",
    "        self.train_metric_values_tc = []\n",
    "        self.train_metric_values_wt = []\n",
    "        self.train_metric_values_et = []\n",
    "\n",
    "        # Validation metrics\n",
    "        self.avg_val_loss_values = []\n",
    "        self.epoch_loss_values = []\n",
    "        self.metric_values = []\n",
    "        self.metric_values_tc = []\n",
    "        self.metric_values_wt = []\n",
    "        self.metric_values_et = []\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def compute_loss(self, outputs, labels):\n",
    "        \"\"\"Combine DiceCE with class-weighted Focal for imbalanced classes\"\"\"\n",
    "        dice_ce_loss = self.ce_loss(outputs, labels)\n",
    "        focal_loss = self.focal_loss(outputs, labels)\n",
    "        \n",
    "        # Balanced weighting with more emphasis on focal for imbalance\n",
    "        total_loss = 0.6 * dice_ce_loss + 0.4 * focal_loss\n",
    "        return total_loss\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        inputs, labels = batch[\"image\"], batch[\"label\"]\n",
    "\n",
    "        # Calculate Train Loss with hybrid approach\n",
    "        outputs = self(inputs)\n",
    "        loss = self.compute_loss(outputs, labels)\n",
    "        \n",
    "        # Log the Train Loss\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "\n",
    "        # Apply sigmoid and threshold\n",
    "        outputs = [self.post_trans(i) for i in decollate_batch(outputs)]\n",
    "        \n",
    "        # Compute Dice\n",
    "        self.dice_metric(y_pred=outputs, y=labels)\n",
    "        self.dice_metric_batch(y_pred=outputs, y=labels)\n",
    "\n",
    "        # Log Train Dice \n",
    "        train_dice = self.dice_metric.aggregate().item()\n",
    "        self.log(\"train_mean_dice\", train_dice, prog_bar=True)\n",
    "\n",
    "        # Store metrics\n",
    "        self.train_metric_values.append(train_dice)\n",
    "        metric_batch = self.dice_metric_batch.aggregate()\n",
    "        self.train_metric_values_tc.append(metric_batch[0].item())\n",
    "        self.train_metric_values_wt.append(metric_batch[1].item())\n",
    "        self.train_metric_values_et.append(metric_batch[2].item())\n",
    "\n",
    "        # Log individual dice metrics\n",
    "        self.log(\"train_tc\", metric_batch[0].item(), prog_bar=True)\n",
    "        self.log(\"train_wt\", metric_batch[1].item(), prog_bar=True)\n",
    "        self.log(\"train_et\", metric_batch[2].item(), prog_bar=True)\n",
    "\n",
    "        # Reset metrics\n",
    "        self.dice_metric.reset()\n",
    "        self.dice_metric_batch.reset()\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        train_loss = self.trainer.logged_metrics[\"train_loss\"].item()\n",
    "        self.train_loss_values.append(train_loss)\n",
    "        \n",
    "        avg_train_loss = sum(self.train_loss_values) / len(self.train_loss_values)\n",
    "        self.log(\"avg_train_loss\", avg_train_loss, prog_bar=True, sync_dist=True)\n",
    "        self.avg_train_loss_values.append(avg_train_loss)\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        val_inputs, val_labels = batch[\"image\"], batch[\"label\"]\n",
    "        \n",
    "        # Multiple overlapping predictions for better accuracy\n",
    "        roi_size = (96, 96, 96)\n",
    "        \n",
    "        # Original prediction\n",
    "        val_outputs = sliding_window_inference(\n",
    "            val_inputs, roi_size=roi_size, sw_batch_size=1, \n",
    "            predictor=self.model, overlap=0.6  # Higher overlap\n",
    "        )\n",
    "        \n",
    "        # Compute loss with hybrid approach\n",
    "        val_loss = self.compute_loss(val_outputs, val_labels)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, sync_dist=True, on_epoch=True)\n",
    "        \n",
    "        val_outputs = [self.post_trans(i) for i in decollate_batch(val_outputs)]    \n",
    "        \n",
    "        # Compute Dice\n",
    "        self.dice_metric(y_pred=val_outputs, y=val_labels)\n",
    "        self.dice_metric_batch(y_pred=val_outputs, y=val_labels)\n",
    "        return {\"val_loss\": val_loss}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        val_dice = self.dice_metric.aggregate().item()\n",
    "        self.metric_values.append(val_dice)\n",
    "\n",
    "        val_loss = self.trainer.logged_metrics[\"val_loss\"].item()\n",
    "        self.epoch_loss_values.append(val_loss)\n",
    "\n",
    "        metric_batch = self.dice_metric_batch.aggregate()\n",
    "        self.metric_values_tc.append(metric_batch[0].item())\n",
    "        self.metric_values_wt.append(metric_batch[1].item())\n",
    "        self.metric_values_et.append(metric_batch[2].item())\n",
    "\n",
    "        # Log validation metrics\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"val_mean_dice\", val_dice, prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"val_tc\", metric_batch[0].item(), prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"val_wt\", metric_batch[1].item(), prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "        self.log(\"val_et\", metric_batch[2].item(), prog_bar=True, on_epoch=True, sync_dist=True)\n",
    "\n",
    "    \n",
    "        if val_dice > self.best_metric:\n",
    "            self.best_metric = val_dice\n",
    "            self.best_metric_epoch = self.current_epoch\n",
    "            torch.save(self.model.state_dict(), \"best_metric_model_swinunetr_v2.pth\")\n",
    "            self.log(\"best_metric\", self.best_metric, sync_dist=True, on_epoch=True)\n",
    "    \n",
    "        # Reset metrics\n",
    "        self.dice_metric.reset()\n",
    "        self.dice_metric_batch.reset()\n",
    "\n",
    "    def on_train_end(self):\n",
    "        print(f\"Train completed, best_metric: {self.best_metric:.4f} at epoch: {self.best_metric_epoch}, \"\n",
    "              f\"tc: {self.metric_values_tc[-1]:.4f}, \"\n",
    "              f\"wt: {self.metric_values_wt[-1]:.4f}, \"\n",
    "              f\"et: {self.metric_values_et[-1]:.4f}.\")\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = AdamW(\n",
    "            self.model.parameters(), \n",
    "            lr=self.hparams.learning_rate, \n",
    "            weight_decay=self.hparams.weight_decay,\n",
    "            betas=(0.9, 0.999),\n",
    "            eps=1e-8\n",
    "        )\n",
    "        \n",
    "        # Warmup + Cosine Annealing\n",
    "        def lr_lambda(epoch):\n",
    "            if epoch < self.hparams.warmup_epochs:\n",
    "                return epoch / self.hparams.warmup_epochs\n",
    "            else:\n",
    "                progress = (epoch - self.hparams.warmup_epochs) / (self.hparams.max_epochs - self.hparams.warmup_epochs)\n",
    "                return 0.5 * (1 + math.cos(math.pi * progress))\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
    "        \n",
    "        return {\n",
    "            \"optimizer\": optimizer,\n",
    "            \"lr_scheduler\": {\n",
    "                \"scheduler\": scheduler,\n",
    "                \"interval\": \"epoch\",\n",
    "                \"frequency\": 1,\n",
    "                \"name\": \"learning_rate\"\n",
    "            }\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-06-14T16:43:14.058290Z",
     "iopub.status.busy": "2025-06-14T16:43:14.057997Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model parameters: 38,109,021\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Path ./wandb_logs/wandb/ wasn't writable, using system temp directory.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.1"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/tmp/wandb/run-20250614_164321-4x7uqlub</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/yokai-re77-north-south-university/brain-tumor-segmentation/runs/4x7uqlub' target=\"_blank\">swinv2-mednext</a></strong> to <a href='https://wandb.ai/yokai-re77-north-south-university/brain-tumor-segmentation' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/yokai-re77-north-south-university/brain-tumor-segmentation' target=\"_blank\">https://wandb.ai/yokai-re77-north-south-university/brain-tumor-segmentation</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/yokai-re77-north-south-university/brain-tumor-segmentation/runs/4x7uqlub' target=\"_blank\">https://wandb.ai/yokai-re77-north-south-university/brain-tumor-segmentation/runs/4x7uqlub</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/pytorch_lightning/trainer/connectors/logger_connector/result.py:434: It is recommended to use `self.log('best_metric', ..., sync_dist=True)` when logging on epoch level in distributed setting to accumulate the metric across devices.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5eb301d25cc644479bad33df19be2d16",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [768, 1, 7, 7, 7], strides() = [343, 1, 49, 7, 1]\n",
      "bucket_view.sizes() = [768, 1, 7, 7, 7], strides() = [343, 343, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "/usr/local/lib/python3.10/dist-packages/torch/autograd/graph.py:825: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.\n",
      "grad.sizes() = [768, 1, 7, 7, 7], strides() = [343, 1, 49, 7, 1]\n",
      "bucket_view.sizes() = [768, 1, 7, 7, 7], strides() = [343, 343, 49, 7, 1] (Triggered internally at ../torch/csrc/distributed/c10d/reducer.cpp:327.)\n",
      "  return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import torch\n",
    "import wandb\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint, LearningRateMonitor, EarlyStopping\n",
    "from pytorch_lightning.strategies import DDPStrategy\n",
    "\n",
    "# Assume your data loaders are already defined\n",
    "train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=4, pin_memory=True, persistent_workers=False)\n",
    "val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=4, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "# Enhanced Logger with more tracking\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"brain-tumor-segmentation\",\n",
    "    name=\"swinv2-mednext\",\n",
    "    log_model=True,\n",
    "    save_dir=\"./wandb_logs\"\n",
    ")\n",
    "\n",
    "# Enhanced Callbacks\n",
    "callbacks = [\n",
    "    ModelCheckpoint(\n",
    "        monitor='val_mean_dice',\n",
    "        mode='max',\n",
    "        save_top_k=3,  # Save top 3 models\n",
    "        save_last=True,\n",
    "        filename='brats-{epoch:02d}-{val_mean_dice:.4f}',\n",
    "        verbose=True,\n",
    "        auto_insert_metric_name=False\n",
    "    ),\n",
    "    LearningRateMonitor(logging_interval='epoch'),\n",
    "    EarlyStopping(\n",
    "        monitor='val_mean_dice',\n",
    "        mode='max',\n",
    "        patience=15,  # Slightly more patience\n",
    "        verbose=True,\n",
    "        min_delta=0.01  # Minimum improvement threshold\n",
    "    ),\n",
    "    Timer(duration=\"00:11:00:00\")\n",
    "]\n",
    "    \n",
    "# Create enhanced model\n",
    "model = BrainTumorSegmentation(\n",
    "    train_loader, val_loader,\n",
    "    feature_size=48,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=2e-5,\n",
    "    warmup_epochs=5,\n",
    "    roi_size=(96, 96, 96),\n",
    "    sw_batch_size=2,\n",
    "    use_v2=True,\n",
    "    depths=(2, 2, 2, 2),\n",
    "    num_heads=(3, 6, 12, 24), \n",
    "    downsample=\"mergingv2\",\n",
    "    ## MedNext Params ##\n",
    "    mednext_exp_r=4,\n",
    "    mednext_kernel_size=7,\n",
    "    mednext_norm_type='group',\n",
    "    mednext_grn=False,  # Enable Global Response Normalization\n",
    "    ## Weighted Params\n",
    "    use_class_weights=True,\n",
    ")\n",
    "\n",
    "print(f\"Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=50,  # Slightly more epochs with better early stopping\n",
    "    precision=\"16-mixed\",\n",
    "    devices=torch.cuda.device_count(),\n",
    "    strategy=\"ddp_notebook\",\n",
    "    # strategy=\"auto\",\n",
    "    accelerator=\"gpu\",\n",
    "    gradient_clip_val=0.5,  # Slightly lower for stability\n",
    "    accumulate_grad_batches=4,\n",
    "    callbacks=callbacks,\n",
    "    logger=wandb_logger,\n",
    "    enable_checkpointing=True,\n",
    "    deterministic=False,\n",
    "    benchmark=True,\n",
    "    log_every_n_steps=5,  # More frequent logging\n",
    "    check_val_every_n_epoch=1,  # Validate once every 10 epochs\n",
    "    limit_val_batches=10,\n",
    "    sync_batchnorm=True,  # Better for multi-GPU\n",
    "    enable_model_summary=True,\n",
    "    profiler=\"simple\"  # Basic profiling for optimization insights\n",
    ")\n",
    "\n",
    "# Train the enhanced model\n",
    "trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End Of Training and Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.832973Z",
     "iopub.status.idle": "2025-06-14T16:09:15.833232Z",
     "shell.execute_reply": "2025-06-14T16:09:15.833127Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plotting train vs validation metrics\n",
    "plt.figure(\"train\", (12, 6))\n",
    "\n",
    "# Plot 1: Epoch Average Loss\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(model.avg_train_loss_values))]\n",
    "y = model.avg_train_loss_values\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y, color=\"red\", label=\"Train Loss\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Train Mean Dice\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Train Mean Dice\")\n",
    "x = [i + 1 for i in range(len(model.train_metric_values))]\n",
    "y = model.train_metric_values\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y, color=\"green\", label=\"Train Dice\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Plotting dice metrics for different categories (TC, WT, ET)\n",
    "plt.figure(\"train\", (18, 6))\n",
    "\n",
    "# Plot 1: Train Mean Dice TC\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Train Mean Dice TC\")\n",
    "x = [i + 1 for i in range(len(model.train_metric_values_tc))]\n",
    "y = model.train_metric_values_tc\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y, color=\"blue\", label=\"Train TC Dice\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot 2: Train Mean Dice WT\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Train Mean Dice WT\")\n",
    "x = [i + 1 for i in range(len(model.train_metric_values_wt))]\n",
    "y = model.train_metric_values_wt\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y, color=\"brown\", label=\"Train WT Dice\")\n",
    "plt.legend()\n",
    "\n",
    "# Plot 3: Train Mean Dice ET\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Train Mean Dice ET\")\n",
    "x = [i + 1 for i in range(len(model.train_metric_values_et))]\n",
    "y = model.train_metric_values_et\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.plot(x, y, color=\"purple\", label=\"Train ET Dice\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.833739Z",
     "iopub.status.idle": "2025-06-14T16:09:15.833980Z",
     "shell.execute_reply": "2025-06-14T16:09:15.833886Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "plt.figure(\"train\", (12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.title(\"Epoch Average Loss\")\n",
    "x = [i + 1 for i in range(len(model.epoch_loss_values))]\n",
    "y = model.epoch_loss_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"red\")\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.title(\"Val Mean Dice\")\n",
    "x = [model.hparams.val_interval * (i + 1) for i in range(len(model.metric_values))]\n",
    "y = model.metric_values\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"green\")\n",
    "plt.show()\n",
    "\n",
    "plt.figure(\"train\", (18, 6))\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.title(\"Val Mean Dice TC\")\n",
    "x = [model.hparams.val_interval * (i + 1) for i in range(len(model.metric_values_tc))]\n",
    "y = model.metric_values_tc\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"blue\")\n",
    "plt.subplot(1, 3, 2)\n",
    "plt.title(\"Val Mean Dice WT\")\n",
    "x = [model.hparams.val_interval * (i + 1) for i in range(len(model.metric_values_wt))]\n",
    "y = model.metric_values_wt\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"brown\")\n",
    "plt.subplot(1, 3, 3)\n",
    "plt.title(\"Val Mean Dice ET\")\n",
    "x = [model.hparams.val_interval * (i + 1) for i in range(len(model.metric_values_et))]\n",
    "y = model.metric_values_et\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.plot(x, y, color=\"purple\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.834536Z",
     "iopub.status.idle": "2025-06-14T16:09:15.834875Z",
     "shell.execute_reply": "2025-06-14T16:09:15.834729Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from monai.inferers import sliding_window_inference\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "# model.model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "# model = model.to(device)  # Add this line to move model to the same device as inputs\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # select one image to evaluate and visualize the model output\n",
    "#     val_input = val_ds[5][\"image\"].unsqueeze(0).to(device)\n",
    "#     roi_size = (96, 96, 96)\n",
    "#     sw_batch_size = 4\n",
    "#     val_output = sliding_window_inference(val_input, roi_size, sw_batch_size, model)\n",
    "#     val_output = post_trans(val_output[0])\n",
    "#     plt.figure(\"image\", (24, 6))\n",
    "#     for i in range(4):\n",
    "#         plt.subplot(1, 4, i + 1)\n",
    "#         plt.title(f\"image channel {i}\")\n",
    "#         plt.imshow(val_ds[5][\"image\"][i, :, :, 72].detach().cpu(), cmap=\"gray\")\n",
    "#     plt.show()\n",
    "#     # visualize the 3 channels label corresponding to this image\n",
    "#     plt.figure(\"label\", (18, 6))\n",
    "#     for i in range(3):\n",
    "#         plt.subplot(1, 3, i + 1)\n",
    "#         plt.title(f\"label channel {i}\")\n",
    "#         plt.imshow(val_ds[5][\"label\"][i, :, :, 72].detach().cpu())\n",
    "#     plt.show()\n",
    "#     # visualize the 3 channels model output corresponding to this image\n",
    "#     plt.figure(\"output\", (18, 6))\n",
    "#     for i in range(3):\n",
    "#         plt.subplot(1, 3, i + 1)\n",
    "#         plt.title(f\"output channel {i}\")\n",
    "#         plt.imshow(val_output[i, :, :, 72].detach().cpu())\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.835650Z",
     "iopub.status.idle": "2025-06-14T16:09:15.835983Z",
     "shell.execute_reply": "2025-06-14T16:09:15.835870Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from monai.inferers import sliding_window_inference\n",
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import torch\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# import os\n",
    "# from monai.transforms import Compose, Activations, AsDiscrete\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "# # Load model and move to device\n",
    "# model.model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Define aesthetically pleasing color scheme\n",
    "# # Using a professionally-designed color palette with better contrast\n",
    "# class_colors = [\n",
    "#     [0.7, 0.7, 0.7, 1],         # Background (neutral gray)\n",
    "#     [0.85, 0.37, 0.35, 0.7],  # Class 1 (rust red - softer and more professional)\n",
    "#     [0.46, 0.78, 0.56, 0.7],  # Class 2 (sage green - easier on the eyes)\n",
    "#     [0.31, 0.51, 0.9, 0.7]    # Class 3 (medium blue - more saturated but not overwhelming)\n",
    "# ]\n",
    "# custom_cmap = ListedColormap(class_colors)\n",
    "\n",
    "# with torch.no_grad():\n",
    "#     # Select one image to evaluate\n",
    "#     val_input = val_ds[8][\"image\"].unsqueeze(0).to(device)\n",
    "#     val_label = val_ds[8][\"label\"]\n",
    "    \n",
    "#     # Inference\n",
    "#     roi_size = (96, 96, 96)\n",
    "#     sw_batch_size = 4\n",
    "#     val_output = sliding_window_inference(val_input, roi_size, sw_batch_size, model)\n",
    "#     val_output = post_trans(val_output[0])\n",
    "    \n",
    "#     # Move tensors to CPU and convert to numpy\n",
    "#     val_input_np = val_input[0, 0].cpu().numpy()  # Shape: (H, W, D)\n",
    "#     val_label_np = val_label.cpu().numpy()  # Shape: (C, H, W, D) where C is number of classes\n",
    "#     val_output_np = val_output.cpu().numpy()  # Shape: (C, H, W, D)\n",
    "    \n",
    "#     # Normalize image for visualization with better contrast\n",
    "#     val_input_np = (val_input_np - val_input_np.min()) / (val_input_np.max() - val_input_np.min())\n",
    "#     val_input_np = (val_input_np * 255).astype(np.uint8)\n",
    "    \n",
    "#     # Determine slice to use (middle slice or 77 if available)\n",
    "#     total_slices = val_input_np.shape[-1]\n",
    "#     middle_slice = total_slices // 2\n",
    "#     slice_idx = 77 if total_slices > 77 else middle_slice\n",
    "#     print(f\"Using slice {slice_idx} out of {total_slices} total slices\")\n",
    "    \n",
    "#     # Create a combined segmentation map for ground truth and prediction\n",
    "#     # Initialize with zeros (background)\n",
    "#     num_classes = val_label_np.shape[0]\n",
    "#     gt_combined = np.zeros((val_label_np.shape[1], val_label_np.shape[2], 4))  # RGBA\n",
    "#     pred_combined = np.zeros((val_output_np.shape[1], val_output_np.shape[2], 4))  # RGBA\n",
    "    \n",
    "#     # Fill in each class with its color\n",
    "#     for c in range(num_classes):\n",
    "#         # For ground truth\n",
    "#         mask = val_label_np[c, :, :, slice_idx]\n",
    "#         for i in range(4):  # RGBA channels\n",
    "#             gt_combined[:, :, i] = np.where(mask > 0, class_colors[c+1][i], gt_combined[:, :, i])\n",
    "        \n",
    "#         # For prediction\n",
    "#         mask = val_output_np[c, :, :, slice_idx]\n",
    "#         for i in range(4):  # RGBA channels\n",
    "#             pred_combined[:, :, i] = np.where(mask > 0, class_colors[c+1][i], pred_combined[:, :, i])\n",
    "    \n",
    "#     # Plot the images with improved styling\n",
    "#     plt.figure(figsize=(18, 6), facecolor='white')\n",
    "    \n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     plt.title(\"Image\", fontsize=14, fontweight='bold')\n",
    "#     plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(1, 3, 2)\n",
    "#     plt.title(\"Ground Truth\", fontsize=14, fontweight='bold')\n",
    "#     plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#     plt.imshow(gt_combined)  # Alpha is already in the array\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     plt.subplot(1, 3, 3)\n",
    "#     plt.title(\"Predicted Segmentation\", fontsize=14, fontweight='bold')\n",
    "#     plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#     plt.imshow(pred_combined)  # Alpha is already in the array\n",
    "#     plt.axis('off')\n",
    "    \n",
    "#     # Add a clearer color legend\n",
    "#     class_names = [\"Tumor Core\", \"Whole Tumor\", \"Enhancing\"]\n",
    "#     legend_patches = [plt.Rectangle((0, 0), 1, 1, fc=class_colors[i+1][:3], alpha=0.7) for i in range(num_classes)]\n",
    "#     plt.figlegend(legend_patches, class_names, loc='lower center', ncol=num_classes, \n",
    "#                  bbox_to_anchor=(0.5, -0.05), fontsize=12, frameon=True, edgecolor='black')\n",
    "    \n",
    "#     plt.tight_layout(pad=1.5)\n",
    "#     plt.subplots_adjust(bottom=0.15)  # Add space for the legend\n",
    "#     plt.show()\n",
    "    \n",
    "#     # Display each class separately with enhanced visualization\n",
    "#     plt.figure(figsize=(15, 5 * num_classes), facecolor='white')\n",
    "    \n",
    "#     # Custom colormaps for each class - more aesthetically pleasing\n",
    "#     class_cmaps = ['RdPu', 'BuGn', 'PuBu']\n",
    "    \n",
    "#     for c in range(num_classes):\n",
    "#         # Ground Truth for this class\n",
    "#         plt.subplot(num_classes, 2, 2*c+1)\n",
    "#         plt.title(f\"Ground Truth - {class_names[c]}\", fontsize=12, fontweight='bold')\n",
    "#         plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#         plt.imshow(val_label_np[c, :, :, slice_idx], cmap=class_cmaps[c], alpha=0.7, vmin=0, vmax=1)\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         # Prediction for this class\n",
    "#         plt.subplot(num_classes, 2, 2*c+2)\n",
    "#         plt.title(f\"Prediction - {class_names[c]}\", fontsize=12, fontweight='bold')\n",
    "#         plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#         plt.imshow(val_output_np[c, :, :, slice_idx], cmap=class_cmaps[c], alpha=0.7, vmin=0, vmax=1)\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         # Add a small colorbar to show intensity\n",
    "#         plt.colorbar(shrink=0.8, ax=plt.gca())\n",
    "    \n",
    "#     plt.tight_layout(pad=2.0)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.836601Z",
     "iopub.status.idle": "2025-06-14T16:09:15.836942Z",
     "shell.execute_reply": "2025-06-14T16:09:15.836828Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# from monai.inferers import sliding_window_inference\n",
    "# from monai.transforms import Compose, Activations, AsDiscrete\n",
    "# import os\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "# # Load model and move to device\n",
    "# model.model.load_state_dict(torch.load(os.path.join(root_dir, \"best_metric_model.pth\")))\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Define aesthetically pleasing color scheme\n",
    "# class_colors = [\n",
    "#     [0.7, 0.7, 0.7, 1],         # Background (neutral gray)\n",
    "#     [0.85, 0.37, 0.35, 0.7],  # Class 1 (rust red - softer and more professional)\n",
    "#     [0.46, 0.78, 0.56, 0.7],  # Class 2 (sage green - easier on the eyes)\n",
    "#     [0.31, 0.51, 0.9, 0.7]    # Class 3 (medium blue - more saturated but not overwhelming)\n",
    "# ]\n",
    "# custom_cmap = ListedColormap(class_colors)\n",
    "\n",
    "# # Randomly select 5 samples\n",
    "# random_indices = random.sample(range(len(val_ds)), 5)\n",
    "\n",
    "# for idx in random_indices:\n",
    "#     with torch.no_grad():\n",
    "#         # Select image to evaluate\n",
    "#         val_input = val_ds[idx][\"image\"].unsqueeze(0).to(device)\n",
    "#         val_label = val_ds[idx][\"label\"]\n",
    "        \n",
    "#         # Inference\n",
    "#         roi_size = (96, 96, 96)\n",
    "#         sw_batch_size = 4\n",
    "#         val_output = sliding_window_inference(val_input, roi_size, sw_batch_size, model)\n",
    "#         val_output = post_trans(val_output[0])\n",
    "        \n",
    "#         # Move tensors to CPU and convert to numpy\n",
    "#         val_input_np = val_input[0, 0].cpu().numpy()  # Shape: (H, W, D)\n",
    "#         val_label_np = val_label.cpu().numpy()  # Shape: (C, H, W, D) where C is number of classes\n",
    "#         val_output_np = val_output.cpu().numpy()  # Shape: (C, H, W, D)\n",
    "        \n",
    "#         # Normalize image for visualization with better contrast\n",
    "#         val_input_np = (val_input_np - val_input_np.min()) / (val_input_np.max() - val_input_np.min())\n",
    "#         val_input_np = (val_input_np * 255).astype(np.uint8)\n",
    "        \n",
    "#         # Determine slice to use (middle slice or 77 if available)\n",
    "#         total_slices = val_input_np.shape[-1]\n",
    "#         middle_slice = total_slices // 2\n",
    "#         slice_idx = 77 if total_slices > 77 else middle_slice\n",
    "#         print(f\"Using slice {slice_idx} out of {total_slices} total slices\")\n",
    "        \n",
    "#         # Create a combined segmentation map for ground truth and prediction\n",
    "#         num_classes = val_label_np.shape[0]\n",
    "#         gt_combined = np.zeros((val_label_np.shape[1], val_label_np.shape[2], 4))  # RGBA\n",
    "#         pred_combined = np.zeros((val_output_np.shape[1], val_output_np.shape[2], 4))  # RGBA\n",
    "        \n",
    "#         # Fill in each class with its color\n",
    "#         for c in range(num_classes):\n",
    "#             # For ground truth\n",
    "#             mask = val_label_np[c, :, :, slice_idx]\n",
    "#             for i in range(4):  # RGBA channels\n",
    "#                 gt_combined[:, :, i] = np.where(mask > 0, class_colors[c+1][i], gt_combined[:, :, i])\n",
    "            \n",
    "#             # For prediction\n",
    "#             mask = val_output_np[c, :, :, slice_idx]\n",
    "#             for i in range(4):  # RGBA channels\n",
    "#                 pred_combined[:, :, i] = np.where(mask > 0, class_colors[c+1][i], pred_combined[:, :, i])\n",
    "        \n",
    "#         # Plot the images with improved styling\n",
    "#         plt.figure(figsize=(18, 6), facecolor='white')\n",
    "        \n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.title(\"Image\", fontsize=14, fontweight='bold')\n",
    "#         plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.title(\"Ground Truth\", fontsize=14, fontweight='bold')\n",
    "#         plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#         plt.imshow(gt_combined)  # Alpha is already in the array\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.title(\"Predicted Segmentation\", fontsize=14, fontweight='bold')\n",
    "#         plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#         plt.imshow(pred_combined)  # Alpha is already in the array\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         # Add a clearer color legend\n",
    "#         class_names = [\"Tumor Core\", \"Whole Tumor\", \"Enhancing\"]\n",
    "#         legend_patches = [plt.Rectangle((0, 0), 1, 1, fc=class_colors[i+1][:3], alpha=0.7) for i in range(num_classes)]\n",
    "#         plt.figlegend(legend_patches, class_names, loc='lower center', ncol=num_classes, \n",
    "#                      bbox_to_anchor=(0.5, -0.05), fontsize=12, frameon=True, edgecolor='black')\n",
    "        \n",
    "#         plt.tight_layout(pad=1.5)\n",
    "#         plt.subplots_adjust(bottom=0.15)  # Add space for the legend\n",
    "#         plt.show()\n",
    "        \n",
    "#         # Display each class separately with enhanced visualization\n",
    "#         plt.figure(figsize=(15, 5 * num_classes), facecolor='white')\n",
    "        \n",
    "#         # Custom colormaps for each class - more aesthetically pleasing\n",
    "#         class_cmaps = ['RdPu', 'BuGn', 'PuBu']\n",
    "        \n",
    "#         for c in range(num_classes):\n",
    "#             # Ground Truth for this class\n",
    "#             plt.subplot(num_classes, 2, 2*c+1)\n",
    "#             plt.title(f\"Ground Truth - {class_names[c]}\", fontsize=12, fontweight='bold')\n",
    "#             plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#             plt.imshow(val_label_np[c, :, :, slice_idx], cmap=class_cmaps[c], alpha=0.7, vmin=0, vmax=1)\n",
    "#             plt.axis('off')\n",
    "            \n",
    "#             # Prediction for this class\n",
    "#             plt.subplot(num_classes, 2, 2*c+2)\n",
    "#             plt.title(f\"Prediction - {class_names[c]}\", fontsize=12, fontweight='bold')\n",
    "#             plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#             plt.imshow(val_output_np[c, :, :, slice_idx], cmap=class_cmaps[c], alpha=0.7, vmin=0, vmax=1)\n",
    "#             plt.axis('off')\n",
    "            \n",
    "#             # Add a small colorbar to show intensity\n",
    "#             plt.colorbar(shrink=0.8, ax=plt.gca())\n",
    "        \n",
    "#         plt.tight_layout(pad=2.0)\n",
    "#         plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.837421Z",
     "iopub.status.idle": "2025-06-14T16:09:15.837683Z",
     "shell.execute_reply": "2025-06-14T16:09:15.837582Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "\n",
    "# def overlay_label_on_image(image, label):\n",
    "#     overlaid_images = []\n",
    "#     for i in range(image.shape[0]):\n",
    "#         overlaid_image = np.zeros_like(image[i])\n",
    "#         # Overlay each label channel onto the corresponding image channel\n",
    "#         for j in range(min(image.shape[0], label.shape[0])):\n",
    "#             overlaid_image[label[j] > 0] = label[j][label[j] > 0]\n",
    "#         overlaid_images.append(overlaid_image)\n",
    "#     return np.stack(overlaid_images)\n",
    "\n",
    "# # Usage:\n",
    "# overlay = overlay_label_on_image(val_ds[1][\"image\"].detach().cpu().numpy(), val_ds[1][\"label\"].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.838251Z",
     "iopub.status.idle": "2025-06-14T16:09:15.838621Z",
     "shell.execute_reply": "2025-06-14T16:09:15.838413Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# overlay.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.839694Z",
     "iopub.status.idle": "2025-06-14T16:09:15.840093Z",
     "shell.execute_reply": "2025-06-14T16:09:15.839931Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# # Iterate over each channel\n",
    "# for i in range(overlay.shape[0]):\n",
    "#     # Create a new figure and a set of subplots with a 3D projection for each channel\n",
    "#     fig = plt.figure()\n",
    "#     ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "#     # Display the 3D volume for the current channel\n",
    "#     ax.voxels(overlay[i])\n",
    "\n",
    "#     # Set labels and title\n",
    "#     ax.set_xlabel('X')\n",
    "#     ax.set_ylabel('Y')\n",
    "#     ax.set_zlabel('Z')\n",
    "#     ax.set_title(f'Overlay Channel {i+1}')\n",
    "\n",
    "#     # Show plot\n",
    "#     plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Test Dataset JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.840829Z",
     "iopub.status.idle": "2025-06-14T16:09:15.841198Z",
     "shell.execute_reply": "2025-06-14T16:09:15.841036Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import json\n",
    "\n",
    "# # Get sorted file paths and file names\n",
    "# file_paths2 = glob.glob('/kaggle/input/brats2023-part-2zip/*')  # Unseen Data \n",
    "# file_paths2.sort()\n",
    "\n",
    "# file_names2 = [os.path.basename(path) for path in file_paths2]  # Extract file names from paths\n",
    "# file_names2.sort()\n",
    "\n",
    "# # Initialize lists for different MRI modalities and segmentation labels\n",
    "# t1c, t1n, t2f, t2w, label = [], [], [], [], []\n",
    "\n",
    "# # Use the total number of files\n",
    "# num_files = len(file_paths2)\n",
    "\n",
    "# # Populate the lists with file paths\n",
    "# for i in range(num_files):\n",
    "#     t1c.append(os.path.join(file_paths2[i], file_names2[i] + '-t1c.nii'))\n",
    "#     t1n.append(os.path.join(file_paths2[i], file_names2[i] + '-t1n.nii'))\n",
    "#     t2f.append(os.path.join(file_paths2[i], file_names2[i] + '-t2f.nii'))\n",
    "#     t2w.append(os.path.join(file_paths2[i], file_names2[i] + '-t2w.nii'))\n",
    "#     label.append(os.path.join(file_paths2[i], file_names2[i] + '-seg.nii'))\n",
    "\n",
    "# # Store in a dictionary with combined image modalities and separate label\n",
    "# file_list = []\n",
    "# for i in range(num_files):\n",
    "#     file_list.append({\n",
    "#         \"image\": [t1c[i], t1n[i], t2f[i], t2w[i]],  # Combine modalities into one \"image\" field\n",
    "#         \"label\": label[i]\n",
    "#     })\n",
    "\n",
    "# file_json = {\n",
    "#     \"testing\": file_list  # Changed key to \"testing\" for clarity\n",
    "# }\n",
    "\n",
    "# # Save to JSON file\n",
    "# file_path = '/kaggle/working/dataset_test.json'\n",
    "# with open(file_path, 'w') as json_file:\n",
    "#     json.dump(file_json, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.842074Z",
     "iopub.status.idle": "2025-06-14T16:09:15.842441Z",
     "shell.execute_reply": "2025-06-14T16:09:15.842282Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Load test dataset\n",
    "# dataset_path = \"/kaggle/working/dataset_test.json\"\n",
    "# with open(dataset_path) as f:\n",
    "#     datalist = json.load(f)[\"testing\"]  # Updated key to match test dataset\n",
    "\n",
    "# ### Run it on 100 samples\n",
    "# datalist = datalist[:40]\n",
    "\n",
    "# test_transform = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"image\", \"label\"]),\n",
    "#         EnsureChannelFirstd(keys=\"image\"),\n",
    "#         EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "#         ConvertLabels(keys=\"label\"),\n",
    "#         Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "#         Spacingd(\n",
    "#             keys=[\"image\", \"label\"],\n",
    "#             pixdim=(1.0, 1.0, 1.0),\n",
    "#             mode=(\"bilinear\", \"nearest\"),\n",
    "#         ),\n",
    "#         NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Create MONAI test dataset\n",
    "# test_ds = Dataset(data=datalist, transform=test_transform)\n",
    "\n",
    "# # Dataloader\n",
    "# test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=3, pin_memory=True, persistent_workers=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.843507Z",
     "iopub.status.idle": "2025-06-14T16:09:15.843886Z",
     "shell.execute_reply": "2025-06-14T16:09:15.843721Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pytorch_lightning as pl\n",
    "# from monai.networks.nets import SwinUNETR\n",
    "# from monai.transforms import Compose, Activations, AsDiscrete\n",
    "# from monai.metrics import DiceMetric\n",
    "# from monai.losses import DiceLoss\n",
    "# from monai.data import DataLoader, Dataset, decollate_batch\n",
    "# from monai.inferers import sliding_window_inference\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "# class BrainTumorSegmentationModel(pl.LightningModule):\n",
    "#     def __init__(self):\n",
    "#         super(BrainTumorSegmentationModel, self).__init__()\n",
    "#         self.model = SwinUNETR(\n",
    "#             img_size=(96, 96, 96),\n",
    "#             in_channels=4,\n",
    "#             out_channels=3,\n",
    "#             feature_size=48,\n",
    "#             use_checkpoint=True,\n",
    "#         )\n",
    "        \n",
    "#         # Load model weights\n",
    "#         self.model.load_state_dict(torch.load(\"/kaggle/input/trained-model-29/pytorch/default/1/swinunetr-29epochs.pth\"))\n",
    "        \n",
    "#         # Post-processing transformations\n",
    "#         self.post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "        \n",
    "#         # Dice metrics for evaluation\n",
    "#         self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "#         self.dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "        \n",
    "#         # Dice loss\n",
    "#         self.dice_loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "\n",
    "#         self.total_loss = 0.0  # To accumulate loss\n",
    "#         self.steps = 0  # Count number of batches\n",
    "        \n",
    "#         # List to store all batch results\n",
    "#         self.all_batch_results = []\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "    \n",
    "#     def test_step(self, batch, batch_idx):\n",
    "#         test_inputs, test_labels = batch[\"image\"].to(self.device), batch[\"label\"].to(self.device)\n",
    "    \n",
    "#         # Inference\n",
    "#         with torch.no_grad():\n",
    "#             test_outputs = sliding_window_inference(test_inputs, roi_size=(96, 96, 96), sw_batch_size=1, predictor=self.model, overlap=0.5)\n",
    "    \n",
    "#         # Compute Dice Loss\n",
    "#         test_loss = self.dice_loss_function(test_outputs, test_labels)\n",
    "    \n",
    "#         # Aggregate Loss\n",
    "#         self.total_loss += test_loss\n",
    "#         self.steps += 1\n",
    "    \n",
    "#         # Post-processing\n",
    "#         test_outputs = [self.post_trans(i) for i in decollate_batch(test_outputs)]\n",
    "    \n",
    "#         # Compute Dice scores\n",
    "#         self.dice_metric(y_pred=test_outputs, y=test_labels)\n",
    "#         self.dice_metric_batch(y_pred=test_outputs, y=test_labels)\n",
    "    \n",
    "#         mean_dice = self.dice_metric.aggregate().item()\n",
    "#         metric_batch = self.dice_metric_batch.aggregate()\n",
    "    \n",
    "#         dice_tc, dice_wt, dice_et = metric_batch[0].item(), metric_batch[1].item(), metric_batch[2].item()\n",
    "    \n",
    "#         self.log(\"Test Loss (Dice)\", test_loss, prog_bar=True)\n",
    "#         self.log(\"Mean Dice\", mean_dice, prog_bar=True)\n",
    "#         self.log(\"Dice TC\", dice_tc, prog_bar=True)\n",
    "#         self.log(\"Dice WT\", dice_wt, prog_bar=True)\n",
    "#         self.log(\"Dice ET\", dice_et, prog_bar=True)\n",
    "    \n",
    "#         # Create batch result dictionary\n",
    "#         batch_result = {\n",
    "#             \"Test Loss (Dice)\": test_loss.item(),\n",
    "#             \"Mean Dice\": mean_dice,\n",
    "#             \"Dice TC\": dice_tc,\n",
    "#             \"Dice WT\": dice_wt,\n",
    "#             \"Dice ET\": dice_et,\n",
    "#         }\n",
    "        \n",
    "#         # Store the batch result\n",
    "#         self.all_batch_results.append(batch_result)\n",
    "    \n",
    "#         # Reset metrics\n",
    "#         self.dice_metric.reset()\n",
    "#         self.dice_metric_batch.reset()\n",
    "    \n",
    "#         # Return metrics as a dictionary\n",
    "#         return batch_result\n",
    "\n",
    "#     def on_test_epoch_end(self):\n",
    "#         \"\"\"Compute and log mean loss over all batches.\"\"\"\n",
    "#         mean_loss = self.total_loss / self.steps if self.steps > 0 else 0\n",
    "#         self.log(\"Mean Test Loss\", mean_loss, prog_bar=True)\n",
    "#         print(f\"Mean Test Loss: {mean_loss:.4f}\")\n",
    "        \n",
    "#         # Calculate and log average metrics across all batches\n",
    "#         avg_metrics = {\n",
    "#             \"Mean Test Loss\": mean_loss.item() if torch.is_tensor(mean_loss) else mean_loss,\n",
    "#             \"Mean Dice\": np.mean([res[\"Mean Dice\"] for res in self.all_batch_results]),\n",
    "#             \"Dice TC\": np.mean([res[\"Dice TC\"] for res in self.all_batch_results]),\n",
    "#             \"Dice WT\": np.mean([res[\"Dice WT\"] for res in self.all_batch_results]),\n",
    "#             \"Dice ET\": np.mean([res[\"Dice ET\"] for res in self.all_batch_results]),\n",
    "#         }\n",
    "        \n",
    "#         print(\"Average Metrics:\", avg_metrics)\n",
    "        \n",
    "#         # Return all results and average metrics\n",
    "#         return {\"batch_results\": self.all_batch_results, \"avg_metrics\": avg_metrics}\n",
    "\n",
    "#     def test_dataloader(self):\n",
    "#         return test_loader  # Ensure you have a DataLoader defined for your test dataset\n",
    "\n",
    "\n",
    "# # Set device (GPU if available, otherwise CPU)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# # Initialize model\n",
    "# model = BrainTumorSegmentationModel()\n",
    "\n",
    "# # Initialize PyTorch Lightning Trainer\n",
    "# trainer = pl.Trainer(\n",
    "#     devices=1,\n",
    "#     accelerator=\"gpu\" if torch.cuda.is_available() else \"cpu\",\n",
    "#     max_epochs=1,  # Since it's inference, set this to 1\n",
    "#     log_every_n_steps=1,\n",
    "# )\n",
    "\n",
    "# # Run inference on test dataset and collect results\n",
    "# test_results = trainer.test(model)\n",
    "\n",
    "# # The test results from trainer.test() will be a list with one dictionary\n",
    "# # If we want to save our detailed results, use:\n",
    "# torch.save(model.all_batch_results, \"test_batch_results.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.844792Z",
     "iopub.status.idle": "2025-06-14T16:09:15.845159Z",
     "shell.execute_reply": "2025-06-14T16:09:15.844998Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import torch\n",
    "\n",
    "# # Load the saved batch results\n",
    "# try:\n",
    "#     batch_results = torch.load(\"test_batch_results.pth\")\n",
    "# except FileNotFoundError:\n",
    "#     print(\"Could not find test_batch_results.pth. Make sure the file exists.\")\n",
    "#     # Create dummy data for demonstration if file doesn't exist\n",
    "#     batch_results = []\n",
    "#     print(\"Using dummy data for demonstration purposes.\")\n",
    "\n",
    "# if batch_results:\n",
    "#     # Calculate average metrics across all batches\n",
    "#     avg_metrics = {\n",
    "#         \"Mean Dice\": np.mean([res[\"Mean Dice\"] for res in batch_results]),\n",
    "#         \"Dice TC\": np.mean([res[\"Dice TC\"] for res in batch_results]),\n",
    "#         \"Dice WT\": np.mean([res[\"Dice WT\"] for res in batch_results]),\n",
    "#         \"Dice ET\": np.mean([res[\"Dice ET\"] for res in batch_results]),\n",
    "#         \"Test Loss (Dice)\": np.mean([res[\"Test Loss (Dice)\"] for res in batch_results])\n",
    "#     }\n",
    "    \n",
    "#     # Create a DataFrame for the Dice metrics (excluding loss for better visualization)\n",
    "#     dice_metrics = {k: v for k, v in avg_metrics.items() if k != \"Test Loss (Dice)\"}\n",
    "    \n",
    "#     # Create a DataFrame for plotting\n",
    "#     metrics_df = pd.DataFrame({\n",
    "#         'Metric': list(dice_metrics.keys()),\n",
    "#         'Value': list(dice_metrics.values())\n",
    "#     })\n",
    "    \n",
    "#     # Set the Seaborn style\n",
    "#     sns.set_style(\"whitegrid\")\n",
    "    \n",
    "#     # # Create figure with two subplots: one for Dice metrics, one for Loss\n",
    "#     # fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6), \n",
    "#     #                                gridspec_kw={'width_ratios': [3, 1]})\n",
    "\n",
    "#     fig, ax1 = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "    \n",
    "#     # 1. Plot Dice metrics\n",
    "#     # Create bar plot with custom color palette\n",
    "#     sns.barplot(x='Metric', y='Value', data=metrics_df, \n",
    "#                 palette='viridis', ax=ax1)\n",
    "    \n",
    "#     # Customize the first subplot\n",
    "#     ax1.set_title('SwinUNETR Test Set Performance', fontsize=16)\n",
    "#     ax1.set_xlabel('Metrics', fontsize=14)\n",
    "#     ax1.set_ylabel('Score', fontsize=14)\n",
    "#     ax1.set_ylim(0, 1.0)  # Dice scores are between 0 and 1\n",
    "#     ax1.set_xticklabels(ax1.get_xticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "#     # Add value labels on top of bars\n",
    "#     for i, value in enumerate(metrics_df['Value']):\n",
    "#         ax1.text(i, value + 0.02, f'{value:.4f}', ha='center', fontsize=12)\n",
    "    \n",
    "#     # # 2. Plot Test Loss\n",
    "#     # loss_value = avg_metrics[\"Test Loss (Dice)\"]\n",
    "#     # ax2.bar(['Test Loss (Dice)'], [loss_value], color='crimson')\n",
    "#     # ax2.set_title('Dice Loss', fontsize=16)\n",
    "#     # ax2.set_ylabel('Loss Value', fontsize=14)\n",
    "#     # ax2.set_ylim(0, min(1.0, loss_value * 1.5))  # Adjust based on loss value\n",
    "#     # ax2.text(0, loss_value + 0.02, f'{loss_value:.4f}', ha='center', fontsize=12)\n",
    "    \n",
    "    \n",
    "#     # Adjust layout\n",
    "#     plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "        \n",
    "#     # Save the figure\n",
    "#     plt.savefig('brain_tumor_segmentation_metrics.png', dpi=300, bbox_inches='tight')\n",
    "    \n",
    "#     # Show the plot\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Knowledge Distillation Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.846193Z",
     "iopub.status.idle": "2025-06-14T16:09:15.846638Z",
     "shell.execute_reply": "2025-06-14T16:09:15.846401Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import time\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "# import pytorch_lightning as pl\n",
    "# from monai.networks.nets import SwinUNETR\n",
    "# from monai.losses import DiceLoss, DiceCELoss\n",
    "# from monai.metrics import DiceMetric\n",
    "# from monai.transforms import Compose, Activations, AsDiscrete\n",
    "# from monai.data import PersistentDataset, list_data_collate, decollate_batch, DataLoader\n",
    "# from monai.inferers import sliding_window_inference\n",
    "# from torch.optim import AdamW\n",
    "# from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "# from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "# from pytorch_lightning.callbacks.timer import Timer\n",
    "# from pytorch_lightning.loggers import WandbLogger\n",
    "# import wandb\n",
    "\n",
    "# class DistillationLoss(nn.Module):\n",
    "#     def __init__(self, alpha=0.5, temperature=2.0, dice_loss=None):\n",
    "#         super().__init__()\n",
    "#         self.alpha = alpha  # Weight for hard/ground-truth loss\n",
    "#         self.temperature = temperature  # Temperature for softening\n",
    "#         self.dice_loss = dice_loss  # The original dice loss\n",
    "#         self.mse_loss = nn.MSELoss()  # Use MSE loss instead of KL for segmentation\n",
    "    \n",
    "#     def forward(self, student_outputs, teacher_outputs, labels):\n",
    "#         # Hard loss (original Dice loss with ground truth)\n",
    "#         hard_loss = self.dice_loss(student_outputs, labels)\n",
    "        \n",
    "#         # Soft loss (MSE between teacher and student logits)\n",
    "#         # Scale logits by temperature\n",
    "#         soft_student = student_outputs / self.temperature\n",
    "#         soft_teacher = teacher_outputs / self.temperature\n",
    "        \n",
    "#         # MSE loss between student and teacher outputs\n",
    "#         soft_loss = self.mse_loss(soft_student, soft_teacher) * (self.temperature ** 2)\n",
    "        \n",
    "#         # Normalize soft loss to be on a similar scale as hard loss\n",
    "#         # This is important to prevent the soft loss from dominating\n",
    "#         # You may need to adjust this scaling factor based on your data\n",
    "#         soft_loss_scaling = 0.01  # Adjust this based on initial loss values\n",
    "#         soft_loss = soft_loss * soft_loss_scaling\n",
    "        \n",
    "#         # Combined loss\n",
    "#         total_loss = self.alpha * hard_loss + (1 - self.alpha) * soft_loss\n",
    "        \n",
    "#         return total_loss, hard_loss, soft_loss\n",
    "\n",
    "# class BrainTumorDistillation(pl.LightningModule):\n",
    "#     def __init__(self, train_loader, val_loader, teacher_model_path, \n",
    "#                  max_epochs=100, val_interval=1, learning_rate=1e-4,\n",
    "#                  alpha=0.5, temperature=4.0, feature_size=24):\n",
    "#         super().__init__()\n",
    "#         self.save_hyperparameters()\n",
    "        \n",
    "#         # Teacher model (frozen)\n",
    "#         self.teacher_model = SwinUNETR(\n",
    "#             img_size=(96, 96, 96),\n",
    "#             in_channels=4,\n",
    "#             out_channels=3,\n",
    "#             feature_size=48,\n",
    "#             use_checkpoint=True,\n",
    "#         )\n",
    "#         # Load teacher weights\n",
    "#         self.teacher_model.load_state_dict(torch.load(teacher_model_path))\n",
    "#         # Freeze teacher model\n",
    "#         for param in self.teacher_model.parameters():\n",
    "#             param.requires_grad = False\n",
    "#         self.teacher_model.eval()\n",
    "        \n",
    "#         # Student model (smaller)\n",
    "#         self.model = SwinUNETR(\n",
    "#             img_size=(96, 96, 96),\n",
    "#             in_channels=4,\n",
    "#             out_channels=3,\n",
    "#             feature_size=feature_size,  # Smaller feature size\n",
    "#             use_checkpoint=True,\n",
    "#         )\n",
    "        \n",
    "#         # Original loss function\n",
    "#         self.dice_loss = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "        \n",
    "#         # Distillation loss\n",
    "#         self.loss_function = DistillationLoss(\n",
    "#             alpha=alpha,\n",
    "#             temperature=temperature,\n",
    "#             dice_loss=self.dice_loss\n",
    "#         )\n",
    "        \n",
    "#         # Metrics\n",
    "#         self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "#         self.dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "\n",
    "#         self.post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "        \n",
    "#         self.best_metric = -1\n",
    "#         self.train_loader = train_loader\n",
    "#         self.val_loader = val_loader\n",
    "\n",
    "#         # Training metrics\n",
    "#         self.avg_train_loss_values = []\n",
    "#         self.train_loss_values = []\n",
    "#         self.train_metric_values = []\n",
    "#         self.train_metric_values_tc = []\n",
    "#         self.train_metric_values_wt = []\n",
    "#         self.train_metric_values_et = []\n",
    "\n",
    "#         # Validation metrics\n",
    "#         self.avg_val_loss_values = []\n",
    "#         self.epoch_loss_values = []\n",
    "#         self.metric_values = []\n",
    "#         self.metric_values_tc = []\n",
    "#         self.metric_values_wt = []\n",
    "#         self.metric_values_et = []\n",
    "        \n",
    "#         # Track distillation-specific metrics\n",
    "#         self.hard_loss_values = []\n",
    "#         self.soft_loss_values = []\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "#     def training_step(self, batch, batch_idx):\n",
    "#         inputs, labels = batch[\"image\"], batch[\"label\"]\n",
    "\n",
    "#         # Get teacher predictions (no gradients)\n",
    "#         with torch.no_grad():\n",
    "#             teacher_outputs = self.teacher_model(inputs)\n",
    "        \n",
    "#         # Get student predictions\n",
    "#         student_outputs = self(inputs)\n",
    "        \n",
    "#         # Calculate losses\n",
    "#         total_loss, hard_loss, soft_loss = self.loss_function(student_outputs, teacher_outputs, labels)\n",
    "        \n",
    "#         # Log the losses\n",
    "#         self.log(\"train_total_loss\", total_loss, prog_bar=True)\n",
    "#         self.log(\"train_hard_loss\", hard_loss, prog_bar=True)\n",
    "#         self.log(\"train_soft_loss\", soft_loss, prog_bar=True)\n",
    "        \n",
    "#         # Store distillation losses\n",
    "#         self.hard_loss_values.append(hard_loss.item())\n",
    "#         self.soft_loss_values.append(soft_loss.item())\n",
    "\n",
    "#         # Apply sigmoid and threshold\n",
    "#         outputs = [self.post_trans(i) for i in decollate_batch(student_outputs)]\n",
    "        \n",
    "#         # Compute Dice\n",
    "#         self.dice_metric(y_pred=outputs, y=labels)\n",
    "#         self.dice_metric_batch(y_pred=outputs, y=labels)\n",
    "\n",
    "#         # Log Train Dice \n",
    "#         train_dice = self.dice_metric.aggregate().item()\n",
    "#         self.log(\"train_mean_dice\", train_dice, prog_bar=True)\n",
    "\n",
    "#         # Store Mean Dice\n",
    "#         self.train_metric_values.append(train_dice)\n",
    "\n",
    "#         # Store the individual dice\n",
    "#         metric_batch = self.dice_metric_batch.aggregate()\n",
    "#         self.train_metric_values_tc.append(metric_batch[0].item())\n",
    "#         self.train_metric_values_wt.append(metric_batch[1].item())\n",
    "#         self.train_metric_values_et.append(metric_batch[2].item())\n",
    "\n",
    "#         # Log the individual dice metrics\n",
    "#         self.log(\"train_tc\", metric_batch[0].item(), prog_bar=True)\n",
    "#         self.log(\"train_wt\", metric_batch[1].item(), prog_bar=True)\n",
    "#         self.log(\"train_et\", metric_batch[2].item(), prog_bar=True)\n",
    "\n",
    "#         if batch_idx == 0 and self.current_epoch % 5 == 0:\n",
    "#             print(f\"Epoch {self.current_epoch}\")\n",
    "#             print(f\"Student output range: {student_outputs.min().item():.4f} to {student_outputs.max().item():.4f}\")\n",
    "#             print(f\"Teacher output range: {teacher_outputs.min().item():.4f} to {teacher_outputs.max().item():.4f}\")\n",
    "#             print(f\"Hard loss: {hard_loss.item():.4f}, Soft loss: {soft_loss.item():.4f}\")\n",
    "\n",
    "#         # Reset metrics for the next batch\n",
    "#         self.dice_metric.reset()\n",
    "#         self.dice_metric_batch.reset()\n",
    "\n",
    "#         return total_loss\n",
    "\n",
    "#     def on_train_epoch_end(self):\n",
    "#         train_loss = self.trainer.logged_metrics[\"train_total_loss\"].item()\n",
    "#         self.train_loss_values.append(train_loss)\n",
    "        \n",
    "#         # Calculate and store average loss per epoch\n",
    "#         avg_train_loss = sum(self.train_loss_values) / len(self.train_loss_values)\n",
    "#         self.log(\"avg_train_loss\", avg_train_loss, prog_bar=True)\n",
    "#         self.avg_train_loss_values.append(avg_train_loss)\n",
    "        \n",
    "#         # Log average distillation losses\n",
    "#         avg_hard_loss = sum(self.hard_loss_values) / len(self.hard_loss_values)\n",
    "#         avg_soft_loss = sum(self.soft_loss_values) / len(self.soft_loss_values)\n",
    "#         self.log(\"avg_hard_loss\", avg_hard_loss)\n",
    "#         self.log(\"avg_soft_loss\", avg_soft_loss)\n",
    "\n",
    "#     def validation_step(self, batch, batch_idx):\n",
    "#         val_inputs, val_labels = batch[\"image\"], batch[\"label\"]\n",
    "        \n",
    "#         # Get teacher predictions for reference\n",
    "#         with torch.no_grad():\n",
    "#             teacher_val_outputs = sliding_window_inference(\n",
    "#                 val_inputs, roi_size=(96, 96, 96), sw_batch_size=1, predictor=self.teacher_model, overlap=0.5\n",
    "#             )\n",
    "        \n",
    "#         # Get student predictions\n",
    "#         student_val_outputs = sliding_window_inference(\n",
    "#             val_inputs, roi_size=(96, 96, 96), sw_batch_size=1, predictor=self.model, overlap=0.5\n",
    "#         )\n",
    "        \n",
    "#         # Compute distillation loss\n",
    "#         val_loss, val_hard_loss, val_soft_loss = self.loss_function(\n",
    "#             student_val_outputs, teacher_val_outputs, val_labels\n",
    "#         )\n",
    "        \n",
    "#         # Log validation losses\n",
    "#         self.log(\"val_total_loss\", val_loss, prog_bar=True, sync_dist=True)\n",
    "#         self.log(\"val_hard_loss\", val_hard_loss, prog_bar=True)\n",
    "#         self.log(\"val_soft_loss\", val_soft_loss, prog_bar=True)\n",
    "        \n",
    "#         # Process student outputs for metrics\n",
    "#         student_post_outputs = [self.post_trans(i) for i in decollate_batch(student_val_outputs)]\n",
    "        \n",
    "#         # Compute Dice metrics\n",
    "#         self.dice_metric(y_pred=student_post_outputs, y=val_labels)\n",
    "#         self.dice_metric_batch(y_pred=student_post_outputs, y=val_labels)\n",
    "        \n",
    "#         # Log validation Dice\n",
    "#         val_dice = self.dice_metric.aggregate().item()\n",
    "#         self.log(\"val_mean_dice\", val_dice, prog_bar=True)\n",
    "        \n",
    "#         # Compare with teacher (for monitoring purposes)\n",
    "#         with torch.no_grad():\n",
    "#             teacher_post_outputs = [self.post_trans(i) for i in decollate_batch(teacher_val_outputs)]\n",
    "#             teacher_dice = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "#             teacher_dice(y_pred=teacher_post_outputs, y=val_labels)\n",
    "#             teacher_dice_score = teacher_dice.aggregate().item()\n",
    "#             self.log(\"teacher_mean_dice\", teacher_dice_score, prog_bar=True)\n",
    "#             self.log(\"teacher_student_gap\", teacher_dice_score - val_dice, prog_bar=True)\n",
    "        \n",
    "#         return {\"val_loss\": val_loss}\n",
    "\n",
    "#     def on_validation_epoch_end(self):\n",
    "#         # Store Dice Mean\n",
    "#         val_dice = self.dice_metric.aggregate().item()\n",
    "#         self.metric_values.append(val_dice)\n",
    "\n",
    "#         # Store Validation Loss \n",
    "#         val_loss = self.trainer.logged_metrics[\"val_total_loss\"].item()\n",
    "#         self.epoch_loss_values.append(val_loss)\n",
    "\n",
    "#         # Calculate and Store avg val loss values\n",
    "#         avg_val_loss = sum(self.epoch_loss_values) / len(self.epoch_loss_values)\n",
    "#         self.log(\"avg_val_loss\", avg_val_loss, prog_bar=True)\n",
    "#         self.avg_val_loss_values.append(avg_val_loss)\n",
    "\n",
    "#         # Store Individual Dice\n",
    "#         metric_batch = self.dice_metric_batch.aggregate()\n",
    "#         self.metric_values_tc.append(metric_batch[0].item())\n",
    "#         self.metric_values_wt.append(metric_batch[1].item())\n",
    "#         self.metric_values_et.append(metric_batch[2].item())\n",
    "\n",
    "#         # Log validation metrics\n",
    "#         self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "#         self.log(\"val_mean_dice\", val_dice, prog_bar=True)\n",
    "#         self.log(\"val_tc\", metric_batch[0].item(), prog_bar=True)\n",
    "#         self.log(\"val_wt\", metric_batch[1].item(), prog_bar=True)\n",
    "#         self.log(\"val_et\", metric_batch[2].item(), prog_bar=True)\n",
    "    \n",
    "#         if val_dice > self.best_metric:\n",
    "#             self.best_metric = val_dice\n",
    "#             self.best_metric_epoch = self.current_epoch\n",
    "#             torch.save(self.model.state_dict(), \"best_distilled_model.pth\")\n",
    "#             self.log(\"best_metric\", self.best_metric)\n",
    "    \n",
    "#         # Reset metrics for the next epoch\n",
    "#         self.dice_metric.reset()\n",
    "#         self.dice_metric_batch.reset()\n",
    "\n",
    "#     def on_train_end(self):\n",
    "#         # Print the best metric and epoch along with individual Dice scores\n",
    "#         print(f\"Train completed, best_metric: {self.best_metric:.4f} at epoch: {self.best_metric_epoch}, \"\n",
    "#               f\"tc: {self.metric_values_tc[-1]:.4f}, \"\n",
    "#               f\"wt: {self.metric_values_wt[-1]:.4f}, \"\n",
    "#               f\"et: {self.metric_values_et[-1]:.4f}.\")\n",
    "        \n",
    "#         # Compare model sizes\n",
    "#         teacher_size = sum(p.numel() for p in self.teacher_model.parameters())\n",
    "#         student_size = sum(p.numel() for p in self.model.parameters())\n",
    "#         print(f\"Teacher model parameters: {teacher_size:,}\")\n",
    "#         print(f\"Student model parameters: {student_size:,}\")\n",
    "#         print(f\"Size reduction: {(1 - student_size/teacher_size)*100:.2f}%\")\n",
    "\n",
    "#     def configure_optimizers(self):\n",
    "#         optimizer = AdamW(self.model.parameters(), lr=self.hparams.learning_rate, weight_decay=1e-5)\n",
    "#         scheduler = CosineAnnealingLR(optimizer, T_max=self.hparams.max_epochs)\n",
    "#         return [optimizer], [scheduler]\n",
    "\n",
    "# # Usage example\n",
    "# def train_distilled_model(teacher_model_path, train_ds, val_ds, feature_size=24):\n",
    "#     # Training setup\n",
    "#     max_epochs = 30\n",
    "#     train_loader = DataLoader(train_ds, batch_size=2, shuffle=True, num_workers=3, pin_memory=True)\n",
    "#     val_loader = DataLoader(val_ds, batch_size=1, shuffle=False, num_workers=3, pin_memory=True)\n",
    "\n",
    "#     # Set up early stopping\n",
    "#     early_stop_callback = EarlyStopping(\n",
    "#        monitor=\"val_total_loss\",\n",
    "#        min_delta=0.00,\n",
    "#        patience=5,\n",
    "#        verbose=True,\n",
    "#        mode='min'\n",
    "#     )\n",
    "#     # Stop training after 10 hours\n",
    "#     timer_callback = Timer(duration=\"00:11:00:00\")\n",
    "\n",
    "#     # Initialize wandb logger\n",
    "#     wandb.init(project=\"brain-tumor-segmentation\", name=\"swinunetr-distillation\")\n",
    "#     wandb_logger = WandbLogger()\n",
    "\n",
    "#     # Initialize and train the model\n",
    "#     model = BrainTumorDistillation(\n",
    "#         train_loader=train_loader, \n",
    "#         val_loader=val_loader, \n",
    "#         teacher_model_path=teacher_model_path,\n",
    "#         max_epochs=max_epochs,\n",
    "#         feature_size=feature_size  # Smaller feature size for student model\n",
    "#     )\n",
    "    \n",
    "#     trainer = pl.Trainer(\n",
    "#         max_epochs=max_epochs,\n",
    "#         devices=1,\n",
    "#         accelerator=\"gpu\",\n",
    "#         precision=\"16-mixed\",\n",
    "#         gradient_clip_val=1.0,\n",
    "#         log_every_n_steps=1,\n",
    "#         callbacks=[early_stop_callback, timer_callback],\n",
    "#         limit_val_batches=5,\n",
    "#         check_val_every_n_epoch=1,\n",
    "#         logger=wandb_logger, \n",
    "#     )\n",
    "\n",
    "#     trainer.fit(model, train_dataloaders=train_loader, val_dataloaders=val_loader)\n",
    "    \n",
    "#     return model\n",
    "\n",
    "\n",
    "# model = train_distilled_model(\n",
    "#     teacher_model_path=\"/kaggle/input/trained-model-29/pytorch/default/1/swinunetr-29epochs.pth\", \n",
    "#     train_ds=train_ds, \n",
    "#     val_ds=val_ds,\n",
    "#     feature_size=24  # Half the original feature size\n",
    "# )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Model Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.847402Z",
     "iopub.status.idle": "2025-06-14T16:09:15.847819Z",
     "shell.execute_reply": "2025-06-14T16:09:15.847632Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import random\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from matplotlib.colors import ListedColormap\n",
    "# from monai.inferers import sliding_window_inference\n",
    "# from monai.transforms import Compose, Activations, AsDiscrete\n",
    "# import os\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "# # Load model and move to device\n",
    "# model.model.load_state_dict(torch.load(os.path.join(root_dir, \"best_distilled_model.pth\")))\n",
    "# model = model.to(device)\n",
    "# model.eval()\n",
    "\n",
    "# # Define aesthetically pleasing color scheme\n",
    "# class_colors = [\n",
    "#     [0.7, 0.7, 0.7, 1],         # Background (neutral gray)\n",
    "#     [0.85, 0.37, 0.35, 0.7],  # Class 1 (rust red - softer and more professional)\n",
    "#     [0.46, 0.78, 0.56, 0.7],  # Class 2 (sage green - easier on the eyes)\n",
    "#     [0.31, 0.51, 0.9, 0.7]    # Class 3 (medium blue - more saturated but not overwhelming)\n",
    "# ]\n",
    "# custom_cmap = ListedColormap(class_colors)\n",
    "\n",
    "# # Randomly select 5 samples\n",
    "# random_indices = random.sample(range(len(val_ds)), 5)\n",
    "\n",
    "# for idx in random_indices:\n",
    "#     with torch.no_grad():\n",
    "#         # Select image to evaluate\n",
    "#         val_input = val_ds[idx][\"image\"].unsqueeze(0).to(device)\n",
    "#         val_label = val_ds[idx][\"label\"]\n",
    "        \n",
    "#         # Inference\n",
    "#         roi_size = (96, 96, 96)\n",
    "#         sw_batch_size = 4\n",
    "#         val_output = sliding_window_inference(val_input, roi_size, sw_batch_size, model)\n",
    "#         val_output = post_trans(val_output[0])\n",
    "        \n",
    "#         # Move tensors to CPU and convert to numpy\n",
    "#         val_input_np = val_input[0, 0].cpu().numpy()  # Shape: (H, W, D)\n",
    "#         val_label_np = val_label.cpu().numpy()  # Shape: (C, H, W, D) where C is number of classes\n",
    "#         val_output_np = val_output.cpu().numpy()  # Shape: (C, H, W, D)\n",
    "        \n",
    "#         # Normalize image for visualization with better contrast\n",
    "#         val_input_np = (val_input_np - val_input_np.min()) / (val_input_np.max() - val_input_np.min())\n",
    "#         val_input_np = (val_input_np * 255).astype(np.uint8)\n",
    "        \n",
    "#         # Determine slice to use (middle slice or 77 if available)\n",
    "#         total_slices = val_input_np.shape[-1]\n",
    "#         middle_slice = total_slices // 2\n",
    "#         slice_idx = 77 if total_slices > 77 else middle_slice\n",
    "#         print(f\"Using slice {slice_idx} out of {total_slices} total slices\")\n",
    "        \n",
    "#         # Create a combined segmentation map for ground truth and prediction\n",
    "#         num_classes = val_label_np.shape[0]\n",
    "#         gt_combined = np.zeros((val_label_np.shape[1], val_label_np.shape[2], 4))  # RGBA\n",
    "#         pred_combined = np.zeros((val_output_np.shape[1], val_output_np.shape[2], 4))  # RGBA\n",
    "        \n",
    "#         # Fill in each class with its color\n",
    "#         for c in range(num_classes):\n",
    "#             # For ground truth\n",
    "#             mask = val_label_np[c, :, :, slice_idx]\n",
    "#             for i in range(4):  # RGBA channels\n",
    "#                 gt_combined[:, :, i] = np.where(mask > 0, class_colors[c+1][i], gt_combined[:, :, i])\n",
    "            \n",
    "#             # For prediction\n",
    "#             mask = val_output_np[c, :, :, slice_idx]\n",
    "#             for i in range(4):  # RGBA channels\n",
    "#                 pred_combined[:, :, i] = np.where(mask > 0, class_colors[c+1][i], pred_combined[:, :, i])\n",
    "        \n",
    "#         # Plot the images with improved styling\n",
    "#         plt.figure(figsize=(18, 6), facecolor='white')\n",
    "        \n",
    "#         plt.subplot(1, 3, 1)\n",
    "#         plt.title(\"Image\", fontsize=14, fontweight='bold')\n",
    "#         plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 3, 2)\n",
    "#         plt.title(\"Ground Truth\", fontsize=14, fontweight='bold')\n",
    "#         plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#         plt.imshow(gt_combined)  # Alpha is already in the array\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         plt.subplot(1, 3, 3)\n",
    "#         plt.title(\"Predicted Segmentation\", fontsize=14, fontweight='bold')\n",
    "#         plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#         plt.imshow(pred_combined)  # Alpha is already in the array\n",
    "#         plt.axis('off')\n",
    "        \n",
    "#         # Add a clearer color legend\n",
    "#         class_names = [\"Tumor Core\", \"Whole Tumor\", \"Enhancing\"]\n",
    "#         legend_patches = [plt.Rectangle((0, 0), 1, 1, fc=class_colors[i+1][:3], alpha=0.7) for i in range(num_classes)]\n",
    "#         plt.figlegend(legend_patches, class_names, loc='lower center', ncol=num_classes, \n",
    "#                      bbox_to_anchor=(0.5, -0.05), fontsize=12, frameon=True, edgecolor='black')\n",
    "        \n",
    "#         plt.tight_layout(pad=1.5)\n",
    "#         plt.subplots_adjust(bottom=0.15)  # Add space for the legend\n",
    "#         plt.show()\n",
    "        \n",
    "#         # Display each class separately with enhanced visualization\n",
    "#         plt.figure(figsize=(15, 5 * num_classes), facecolor='white')\n",
    "        \n",
    "#         # Custom colormaps for each class - more aesthetically pleasing\n",
    "#         class_cmaps = ['RdPu', 'BuGn', 'PuBu']\n",
    "        \n",
    "#         for c in range(num_classes):\n",
    "#             # Ground Truth for this class\n",
    "#             plt.subplot(num_classes, 2, 2*c+1)\n",
    "#             plt.title(f\"Ground Truth - {class_names[c]}\", fontsize=12, fontweight='bold')\n",
    "#             plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#             plt.imshow(val_label_np[c, :, :, slice_idx], cmap=class_cmaps[c], alpha=0.7, vmin=0, vmax=1)\n",
    "#             plt.axis('off')\n",
    "            \n",
    "#             # Prediction for this class\n",
    "#             plt.subplot(num_classes, 2, 2*c+2)\n",
    "#             plt.title(f\"Prediction - {class_names[c]}\", fontsize=12, fontweight='bold')\n",
    "#             plt.imshow(val_input_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#             plt.imshow(val_output_np[c, :, :, slice_idx], cmap=class_cmaps[c], alpha=0.7, vmin=0, vmax=1)\n",
    "#             plt.axis('off')\n",
    "            \n",
    "#             # Add a small colorbar to show intensity\n",
    "#             plt.colorbar(shrink=0.8, ax=plt.gca())\n",
    "        \n",
    "#         plt.tight_layout(pad=2.0)\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.848542Z",
     "iopub.status.idle": "2025-06-14T16:09:15.848945Z",
     "shell.execute_reply": "2025-06-14T16:09:15.848774Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# from monai.inferers import sliding_window_inference\n",
    "\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "\n",
    "\n",
    "# model.model.load_state_dict(torch.load(os.path.join(root_dir, \"best_distilled_model.pth\")))\n",
    "# model = model.to(device)  # Add this line to move model to the same device as inputs\n",
    "# model.eval()\n",
    "# with torch.no_grad():\n",
    "#     # select one image to evaluate and visualize the model output\n",
    "#     val_input = val_ds[5][\"image\"].unsqueeze(0).to(device)\n",
    "#     roi_size = (96, 96, 96)\n",
    "#     sw_batch_size = 4\n",
    "#     val_output = sliding_window_inference(val_input, roi_size, sw_batch_size, model)\n",
    "#     val_output = post_trans(val_output[0])\n",
    "#     plt.figure(\"image\", (24, 6))\n",
    "#     for i in range(4):\n",
    "#         plt.subplot(1, 4, i + 1)\n",
    "#         plt.title(f\"image channel {i}\")\n",
    "#         plt.imshow(val_ds[5][\"image\"][i, :, :, 72].detach().cpu(), cmap=\"gray\")\n",
    "#     plt.show()\n",
    "#     # visualize the 3 channels label corresponding to this image\n",
    "#     plt.figure(\"label\", (18, 6))\n",
    "#     for i in range(3):\n",
    "#         plt.subplot(1, 3, i + 1)\n",
    "#         plt.title(f\"label channel {i}\")\n",
    "#         plt.imshow(val_ds[5][\"label\"][i, :, :, 72].detach().cpu())\n",
    "#     plt.show()\n",
    "#     # visualize the 3 channels model output corresponding to this image\n",
    "#     plt.figure(\"output\", (18, 6))\n",
    "#     for i in range(3):\n",
    "#         plt.subplot(1, 3, i + 1)\n",
    "#         plt.title(f\"output channel {i}\")\n",
    "#         plt.imshow(val_output[i, :, :, 72].detach().cpu())\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Student Model Test Performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare Test JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.849657Z",
     "iopub.status.idle": "2025-06-14T16:09:15.850064Z",
     "shell.execute_reply": "2025-06-14T16:09:15.849892Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob\n",
    "# import json\n",
    "\n",
    "# # Get sorted file paths and file names\n",
    "# file_paths2 = glob.glob('/kaggle/input/brats2023-part-2zip/*')  # Unseen Data \n",
    "# file_paths2.sort()\n",
    "\n",
    "# file_names2 = [os.path.basename(path) for path in file_paths2]  # Extract file names from paths\n",
    "# file_names2.sort()\n",
    "\n",
    "# # Initialize lists for different MRI modalities and segmentation labels\n",
    "# t1c, t1n, t2f, t2w, label = [], [], [], [], []\n",
    "\n",
    "# # Use the total number of files\n",
    "# num_files = len(file_paths2)\n",
    "\n",
    "# # Populate the lists with file paths\n",
    "# for i in range(num_files):\n",
    "#     t1c.append(os.path.join(file_paths2[i], file_names2[i] + '-t1c.nii'))\n",
    "#     t1n.append(os.path.join(file_paths2[i], file_names2[i] + '-t1n.nii'))\n",
    "#     t2f.append(os.path.join(file_paths2[i], file_names2[i] + '-t2f.nii'))\n",
    "#     t2w.append(os.path.join(file_paths2[i], file_names2[i] + '-t2w.nii'))\n",
    "#     label.append(os.path.join(file_paths2[i], file_names2[i] + '-seg.nii'))\n",
    "\n",
    "# # Store in a dictionary with combined image modalities and separate label\n",
    "# file_list = []\n",
    "# for i in range(num_files):\n",
    "#     file_list.append({\n",
    "#         \"image\": [t1c[i], t1n[i], t2f[i], t2w[i]],  # Combine modalities into one \"image\" field\n",
    "#         \"label\": label[i]\n",
    "#     })\n",
    "\n",
    "# file_json = {\n",
    "#     \"testing\": file_list  # Changed key to \"testing\" for clarity\n",
    "# }\n",
    "\n",
    "# # Save to JSON file\n",
    "# file_path = '/kaggle/working/dataset_test.json'\n",
    "# with open(file_path, 'w') as json_file:\n",
    "#     json.dump(file_json, json_file, indent=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Dataloader "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.850903Z",
     "iopub.status.idle": "2025-06-14T16:09:15.851291Z",
     "shell.execute_reply": "2025-06-14T16:09:15.851121Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# # Load test dataset\n",
    "# dataset_path = \"/kaggle/working/dataset_test.json\"\n",
    "# with open(dataset_path) as f:\n",
    "#     datalist = json.load(f)[\"testing\"]  # Updated key to match test dataset\n",
    "\n",
    "# ### Run it on 100 samples\n",
    "# datalist = datalist[:40]\n",
    "\n",
    "# test_transform = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"image\", \"label\"]),\n",
    "#         EnsureChannelFirstd(keys=\"image\"),\n",
    "#         EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "#         ConvertLabels(keys=\"label\"),\n",
    "#         Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "#         Spacingd(\n",
    "#             keys=[\"image\", \"label\"],\n",
    "#             pixdim=(1.0, 1.0, 1.0),\n",
    "#             mode=(\"bilinear\", \"nearest\"),\n",
    "#         ),\n",
    "#         NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Create MONAI test dataset\n",
    "# test_ds = Dataset(data=datalist, transform=test_transform)\n",
    "\n",
    "# # Dataloader\n",
    "# test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=3, pin_memory=True, persistent_workers=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Pipeline (Student) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.852164Z",
     "iopub.status.idle": "2025-06-14T16:09:15.852557Z",
     "shell.execute_reply": "2025-06-14T16:09:15.852368Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import pytorch_lightning as pl\n",
    "# from monai.networks.nets import SwinUNETR\n",
    "# from monai.transforms import Compose, Activations, AsDiscrete\n",
    "# from monai.metrics import DiceMetric\n",
    "# from monai.losses import DiceLoss\n",
    "# from monai.data import DataLoader, Dataset, decollate_batch\n",
    "# from monai.inferers import sliding_window_inference\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pandas as pd\n",
    "# import seaborn as sns\n",
    "# import numpy as np\n",
    "\n",
    "# class BrainTumorSegmentationModel(pl.LightningModule):\n",
    "#     def __init__(self):\n",
    "#         super(BrainTumorSegmentationModel, self).__init__()\n",
    "#         self.model = SwinUNETR(\n",
    "#             img_size=(96, 96, 96),\n",
    "#             in_channels=4,\n",
    "#             out_channels=3,\n",
    "#             feature_size=24,\n",
    "#             use_checkpoint=True,\n",
    "#         )\n",
    "        \n",
    "#         # Load model weights\n",
    "#         self.model.load_state_dict(torch.load(\"/kaggle/input/swin_distilled_model/pytorch/default/1/best_distilled_model.pth\"))\n",
    "        \n",
    "#         # Post-processing transformations\n",
    "#         self.post_trans = Compose([Activations(sigmoid=True), AsDiscrete(threshold=0.5)])\n",
    "        \n",
    "#         # Dice metrics for evaluation\n",
    "#         self.dice_metric = DiceMetric(include_background=True, reduction=\"mean\")\n",
    "#         self.dice_metric_batch = DiceMetric(include_background=True, reduction=\"mean_batch\")\n",
    "        \n",
    "#         # Dice loss\n",
    "#         self.dice_loss_function = DiceLoss(smooth_nr=0, smooth_dr=1e-5, squared_pred=True, to_onehot_y=False, sigmoid=True)\n",
    "\n",
    "#         self.total_loss = 0.0  # To accumulate loss\n",
    "#         self.steps = 0  # Count number of batches\n",
    "        \n",
    "#         # List to store all batch results\n",
    "#         self.all_batch_results = []\n",
    "        \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "\n",
    "# model = BrainTumorSegmentationModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.853370Z",
     "iopub.status.idle": "2025-06-14T16:09:15.853767Z",
     "shell.execute_reply": "2025-06-14T16:09:15.853594Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# for name, module in model.model.named_modules():\n",
    "#     print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.854697Z",
     "iopub.status.idle": "2025-06-14T16:09:15.855101Z",
     "shell.execute_reply": "2025-06-14T16:09:15.854930Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import json\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# from monai.networks.nets import SwinUNETR\n",
    "# from monai.data import Dataset, DataLoader\n",
    "# from monai.transforms import (\n",
    "#     Compose, LoadImaged, EnsureChannelFirstd, EnsureTyped,\n",
    "#     Orientationd, Spacingd, NormalizeIntensityd, Resized\n",
    "# )\n",
    "# from monai.visualize import GradCAM\n",
    "# from monai.transforms import Resize\n",
    "# from monai.utils.misc import set_determinism\n",
    "\n",
    "# set_determinism(42)\n",
    "\n",
    "# # --- Preprocessing ---\n",
    "# def load_datalist(path, max_samples=40):\n",
    "#     with open(path) as f:\n",
    "#         return json.load(f)[\"testing\"][:max_samples]\n",
    "\n",
    "# def get_transforms():\n",
    "#     return Compose([\n",
    "#         LoadImaged(keys=[\"image\", \"label\"]),\n",
    "#         EnsureChannelFirstd(keys=[\"image\", \"label\"]),\n",
    "#         EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "#         Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "#         Spacingd(keys=[\"image\", \"label\"], pixdim=(1, 1, 1), mode=(\"bilinear\", \"nearest\")),\n",
    "#         NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "#         Resized(keys=[\"image\", \"label\"], spatial_size=(96, 96, 96), mode=[\"trilinear\", \"nearest\"]),\n",
    "#     ])\n",
    "\n",
    "# # --- Model ---\n",
    "# def build_model():\n",
    "#     return SwinUNETR(\n",
    "#         img_size=(96, 96, 96),\n",
    "#         in_channels=4,\n",
    "#         out_channels=3,\n",
    "#         feature_size=24,\n",
    "#         use_checkpoint=True\n",
    "#     )\n",
    "\n",
    "# def load_weights(model, path):\n",
    "#     device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "#     model.load_state_dict(torch.load(path, map_location=device))\n",
    "#     return model.eval().to(device), device\n",
    "\n",
    "# # --- GradCAM Utils ---\n",
    "# def normalize(cam):\n",
    "#     return (cam - cam.min()) / (cam.max() - cam.min() + 1e-5)\n",
    "\n",
    "# import torch.nn.functional as F\n",
    "# def resize_cam(cam, target_shape):\n",
    "#     if cam.ndim == 4:  # [B, H, W, D]\n",
    "#         cam = cam.unsqueeze(1)  # -> [B, 1, H, W, D]\n",
    "#     elif cam.ndim == 3:  # [H, W, D]\n",
    "#         cam = cam[None, None]   # -> [1, 1, H, W, D]\n",
    "\n",
    "#     cam = F.interpolate(cam, size=target_shape, mode=\"trilinear\", align_corners=False)\n",
    "#     return normalize(cam)\n",
    "\n",
    "\n",
    "# # --- Visualization ---\n",
    "# def show_cam_overlay(image, cam, title, channel_idx=3, channel_name=\"T2\"):\n",
    "#     mid = image.shape[2] // 2\n",
    "#     plt.figure(figsize=(12, 5))\n",
    "#     plt.subplot(1, 2, 1)\n",
    "#     plt.imshow(image[channel_idx, :, mid, :], cmap=\"gray\")\n",
    "#     plt.title(f\"Original - {channel_name}\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.subplot(1, 2, 2)\n",
    "#     plt.imshow(image[channel_idx, :, mid, :], cmap=\"gray\")\n",
    "#     plt.imshow(cam[0, :, mid, :], cmap=\"jet_r\", alpha=0.5)\n",
    "#     plt.title(f\"Grad-CAM Overlay - {channel_name}\")\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.suptitle(title)\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # --- Main Execution ---\n",
    "# def run_gradcam(\n",
    "#     dataset_path,\n",
    "#     checkpoint_path,\n",
    "#     sample_idx=0,\n",
    "#     target_class=1\n",
    "# ):\n",
    "#     datalist = load_datalist(dataset_path)\n",
    "#     dataset = Dataset(data=datalist, transform=get_transforms())\n",
    "#     loader = DataLoader(dataset, batch_size=1)\n",
    "\n",
    "#     model = build_model()\n",
    "#     model, device = load_weights(model, checkpoint_path)\n",
    "\n",
    "#     # Get sample\n",
    "#     sample = dataset[sample_idx]\n",
    "#     image = sample[\"image\"].unsqueeze(0).to(device)\n",
    "\n",
    "#     # GradCAM\n",
    "#     gradcam = GradCAM(nn_module=model, target_layers=\"encoder1.layer.norm3\")\n",
    "#     cam_raw = gradcam(x=image, class_idx=target_class)\n",
    "\n",
    "#     # Resize and Normalize CAM\n",
    "#     cam = resize_cam(cam_raw, image.shape[2:])\n",
    "\n",
    "#     # Visualization\n",
    "#     input_np = image[0].cpu().numpy()\n",
    "#     cam_np = cam[0].cpu().numpy()\n",
    "#     class_names = [\"Tumor Core\", \"Whole Tumor\", \"Enhancing Tumor\"]\n",
    "#     show_cam_overlay(input_np, cam_np, f\"Grad-CAM: {class_names[target_class]}\")\n",
    "\n",
    "#     return input_np, cam_np\n",
    "\n",
    "# # Example usage\n",
    "# if __name__ == \"__main__\":\n",
    "#     run_gradcam(\n",
    "#         dataset_path=\"/kaggle/working/dataset_test.json\",\n",
    "#         checkpoint_path=\"/kaggle/input/swin_distilled_model/pytorch/default/1/best_distilled_model.pth\",\n",
    "#         sample_idx=17,\n",
    "#         target_class=0\n",
    "#     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.855916Z",
     "iopub.status.idle": "2025-06-14T16:09:15.856284Z",
     "shell.execute_reply": "2025-06-14T16:09:15.856123Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# import json\n",
    "# import torch\n",
    "# import numpy as np\n",
    "# import matplotlib.pyplot as plt\n",
    "# import pytorch_lightning as pl\n",
    "# from monai.networks.nets import SwinUNETR\n",
    "# from monai.data import Dataset, DataLoader\n",
    "# from monai.transforms import (\n",
    "#     Compose, LoadImaged, EnsureChannelFirstd, EnsureTyped,\n",
    "#     Orientationd, Spacingd, NormalizeIntensityd, Resized,\n",
    "#     MapTransform\n",
    "# )\n",
    "# from monai.visualize import GradCAM\n",
    "# from monai.utils.misc import set_determinism\n",
    "\n",
    "# # Set determinism for reproducibility\n",
    "# set_determinism(42)\n",
    "\n",
    "# # Create model\n",
    "# def create_model():\n",
    "#     return SwinUNETR(\n",
    "#         img_size=(96, 96, 96),\n",
    "#         in_channels=4,\n",
    "#         out_channels=3,\n",
    "#         feature_size=24,\n",
    "#         use_checkpoint=True,\n",
    "#     )\n",
    "\n",
    "# # Load test dataset\n",
    "# dataset_path = \"/kaggle/working/dataset_test.json\"\n",
    "# with open(dataset_path) as f:\n",
    "#     datalist = json.load(f)[\"testing\"]\n",
    "\n",
    "# # Limit to 40 samples\n",
    "# datalist = datalist[:40]\n",
    "\n",
    "# # Test transforms with Resized\n",
    "# test_transform = Compose(\n",
    "#     [\n",
    "#         LoadImaged(keys=[\"image\", \"label\"]),\n",
    "#         EnsureChannelFirstd(keys=\"image\"),\n",
    "#         EnsureTyped(keys=[\"image\", \"label\"]),\n",
    "#         Orientationd(keys=[\"image\", \"label\"], axcodes=\"RAS\"),\n",
    "#         Res enized(\n",
    "#             keys=[\"image\", \"label\"],\n",
    "#             spatial_size=(96, 96, 96),\n",
    "#             mode=(\"trilinear\", \"nearest\"),\n",
    "#         ),\n",
    "#         NormalizeIntensityd(keys=\"image\", nonzero=True, channel_wise=True),\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "# # Create test dataset and dataloader\n",
    "# test_ds = Dataset(data=datalist, transform=test_transform)\n",
    "# test_loader = DataLoader(test_ds, batch_size=1, shuffle=False, num_workers=3, pin_memory=True, persistent_workers=False)\n",
    "\n",
    "# # Initialize model and load checkpoint\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = create_model().to(device)\n",
    "# checkpoint_path = \"/kaggle/input/swin_distilled_model/pytorch/default/1/best_distilled_model.pth\"\n",
    "# checkpoint = torch.load(checkpoint_path, map_location=device)\n",
    "# model.load_state_dict(checkpoint[\"state_dict\"] if \"state_dict\" in checkpoint else checkpoint)\n",
    "# model.eval()\n",
    "\n",
    "# # Inspect model architecture (optional, for debugging)\n",
    "# # for name, module in model.named_modules():\n",
    "# #     print(name)\n",
    "\n",
    "# # Initialize Grad-CAM with a new target layer\n",
    "# target_layer = \"encoder1\"  # Use the entire encoder1 block\n",
    "# grad_cam = GradCAM(nn_module=model, target_layers=target_layer)\n",
    "\n",
    "# # Function to visualize Grad-CAM\n",
    "# def visualize_gradcam(image, cam_result, slice_idx=48, alpha=0.5):\n",
    "#     # Convert to numpy for visualization\n",
    "#     image_np = image.cpu().detach().numpy()[0, 0]  # Select first channel\n",
    "#     cam_np = cam_result.cpu().detach().numpy()[0, 0]  # Select first channel of CAM\n",
    "\n",
    "#     # Normalize CAM for visualization\n",
    "#     cam_np = (cam_np - cam_np.min()) / (cam_np.max() - cam_np.min() + 1e-8)\n",
    "\n",
    "#     # Plot\n",
    "#     plt.figure(figsize=(12, 4))\n",
    "\n",
    "#     # Original image slice\n",
    "#     plt.subplot(1, 3, 1)\n",
    "#     plt.imshow(image_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#     plt.title(\"Original Image\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     # Grad-CAM heatmap\n",
    "#     plt.subplot(1, 3, 2)\n",
    "#     plt.imshow(cam_np[:, :, slice_idx], cmap=\"jet\")\n",
    "#     plt.title(\"Grad-CAM Heatmap\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     # Overlay\n",
    "#     plt.subplot(1, 3, 3)\n",
    "#     plt.imshow(image_np[:, :, slice_idx], cmap=\"gray\")\n",
    "#     plt.imshow(cam_np[:, :, slice_idx], cmap=\"jet_r\", alpha=alpha)\n",
    "#     plt.title(\"Overlay\")\n",
    "#     plt.axis(\"off\")\n",
    "\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "# # Run Grad-CAM on one sample\n",
    "# for i, batch in enumerate(test_loader):\n",
    "#     # Get image and label\n",
    "#     image = batch[\"image\"].to(device)  # Shape: [1, 4, 96, 96, 96]\n",
    "#     label = batch[\"label\"].to(device)  # Shape: [1, 3, 96, 96, 96]\n",
    "\n",
    "#     # Enable gradient tracking for the input\n",
    "#     image.requires_grad_(True)\n",
    "\n",
    "#     # Compute Grad-CAM\n",
    "#     class_index = 1\n",
    "#     cam_result = grad_cam(x=image, class_idx=class_index)  # Shape: [1, 1, 96, 96, 96]\n",
    "\n",
    "#     # Visualize\n",
    "#     print(f\"Visualizing Grad-CAM for sample {i+1}\")\n",
    "#     visualize_gradcam(image, cam_result)\n",
    "\n",
    "#     # Process only one sample for demonstration\n",
    "#     break\n",
    "\n",
    "# print(\"Grad-CAM visualization complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-06-14T16:09:15.857173Z",
     "iopub.status.idle": "2025-06-14T16:09:15.857562Z",
     "shell.execute_reply": "2025-06-14T16:09:15.857380Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# import os\n",
    "# os._exit(00)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 3638326,
     "sourceId": 6322179,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 3638341,
     "sourceId": 6322203,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
